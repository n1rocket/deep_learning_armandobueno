{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Módulo 8. Aprendizaje no supervisado\n",
        "\n",
        "¡Bienvenidos al octavo módulo! Durante todo el curso, hemos trabajado con metodologías basadas en aprendizaje supervisado. En el proceso de entrenamiento, le introduciamos a la red la verdad absoluta (GT) para que intente reducir en lo máximo posible el error obtenido mediante el conocido Gradient Descent. Sin embargo, existen otras metodologías que trabajan de otra forma y que no necesitan una etiqueta para su aprendizaje. El objetivo de esta sesión es que conozcamos y nos familiarizaremos con algunas de las más conocidas y sus tareas asociadas.\n",
        "\n",
        "En este módulo estudiaremos lo siguiente:\n",
        "\n",
        "1. **¿Qué es el aprendizaje no supervisado?**\n",
        "\n",
        "2. **Autoencoders**\n",
        "\n",
        " <ul>\n",
        "   2.1 Reducción de la dimensionalidad\n",
        "\n",
        "   2.2 Eliminación de ruido\n",
        "\n",
        "   2.3 Detección de anomalías\n",
        "   </ul>\n",
        "\n",
        "3. **Variational Autoencoders**\n",
        "\n",
        "3. **Generative Adversarial Neural Network (GANs)**\n"
      ],
      "metadata": {
        "id": "IYfbn0YCgm5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. ¿Qué es el aprendizaje no supervisado?** \n",
        "\n",
        "Los algoritmos de machine y deep learning son frecuentemente clasificados en dos categorías: aprendizaje supervisado y aprendizaje no supervisado. La diferencia entre estas categorías depende del tipo de datos con el que estemos lidiando: el aprendizaje supervisado lidia con datos etiquetados (la etiqueta es lo que estamos intentando predecir) mientras que el aprendiza no supervisado lidia con datos no etiquetados.\n",
        "\n",
        "Para ilustrar las distinciones entre aprendizaje supervisado y no supervisado, tomemos el ejemplo de las imágenes. El aprendizaje supervisado se ocupa del caso en el que tenemos las imágenes y las etiquetas de lo que está contenido en la imagen (por ejemplo, gato). El aprendizaje no supervisado se ocupa del caso en el que solo tenemos las imágenes.\n",
        "\n",
        "\n",
        "<center> <img src=\"https://i.ibb.co/02W8cSb/aprendizaje-no-supervisado.png\" border=\"0\"> </center> </a>\n",
        "\n",
        "**¿Por qué utilizar aprendizaje no supervisado?**\n",
        "\n",
        "\n",
        "* Mucho más datos sin etiquetar que etiquetados:\n",
        "    1.   Grandes datos --> mejores modelos\n",
        "    2.   Lo ideal es no pagar por anotaciones\n",
        "    3. Descubrir la estructura de los datos: ¿Cuáles son las características importantes en el conjunto de datos?\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "vGEBRfQPiT2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Autoencoders** \n",
        "Los autoencoders son una técnica de aprendizaje no supervisada en la que aprovechamos las redes neuronales para la tarea de aprendizaje de representación. Específicamente, el objetivo es diseñar una arquitectura de red neuronal tal que impongamos un cuello de botella (bottleneck) en la red que obligue a generar una representación comprimida, con una dimension menor de la entrada original. Por lo tanto, el autoencoder, internamente comprime los datos de entrada en un espacio latente y posteriormente, reconstruye los datos de entrada de ese espacio latente. Idealmente, el output del autoencoder debe ser idéntico a la entrada.\n",
        "\n",
        "Por lo general, un algoritmo basado en autoencoder tiene dos componentes o subredes:\n",
        "\n",
        " \n",
        "\n",
        "*   **Codificador (Encoder)**: acepta los datos de entrada y los comprime en el espacio latente. Si denotamos nuestros datos de entrada como x y el codificador como E, entonces la representación del espacio latente de salida, s, sería s=(x).\n",
        "*   **Decodificador (Decoder)**: El decodificador es responsable de aceptar las representaciones de espacio latente y luego reconstruir la entrada original. Si denotamos la función del decodificador como D y la salida del detector como o, entonces podemos representar el decodificador como o = D(s).\n",
        "\n",
        "\n",
        "<center> <a href=\"https://ibb.co/hWqjG7\"><img src=\"https://i.ibb.co/kDn442m/autoencoder.png\" alt=\"IA engloba ML, que a su vez engloba DL\" border=\"0\"> </center> </a>\n",
        "\n",
        "\n",
        "Durante el proceso de entrenamiento, el objetivo de un autoencoder es entrenar una red que pueda aprender a reconstruir nuestros datos de entrada. Por tanto, la función de pérdidas que se suele utilizar en este caso es el MSE, donde comparamos los datos de entrada con los datos generados. El objetivo es que el error generado sea el mínimo posible. El verdadero valor del codificador automático vive dentro de esa **representación de espacio latente**. Tened en cuenta que los codificadores automáticos comprimen nuestros datos de entrada y, más concretamente, cuando entrenamos codificadores automáticos, lo que realmente nos importa es el codificador, E, y la representación del espacio latente, s = E(x).\n",
        "\n",
        "El decodificador, o = D(s), se usa para entrenar el codificador automático de extremo a extremo, pero en aplicaciones prácticas, a menudo (pero no siempre) nos preocupamos más por el codificador y el espacio latente.\n",
        "\n",
        "Y bien, ¿para qué  se utilizan los autoencoder?\n",
        "\n",
        "Las aplicaciones más típicas son las siguientes:\n",
        "\n",
        "\n",
        "*   **Reducción de dimensionalidad** (es decir, como una PCA pero más potente/inteligente).\n",
        "*   **Eliminación de ruido** (p. ej., eliminación de ruido y preprocesamiento de imágenes para mejorar la precisión de OCR).\n",
        "\n",
        "* **Detección de anomalías/valores atípicos** (p. ej., detectar puntos de datos mal etiquetados en un conjunto de datos o detectar cuándo un punto de datos de entrada se encuentra fuera de nuestra distribución típica de datos).\n",
        "\n",
        "\n",
        "Fuera del campo de computer vision, se utilizarán codificadores automáticos aplicados al procesamiento del lenguaje natural (NLP) y problemas de comprensión de texto, incluida la comprensión del significado semántico de las palabras, la construcción de incrustaciones de palabras e incluso el resumen de texto.\n",
        "\n",
        "Ahora que conocemos un poco más como funcionan los autoencoder vamos a ponernos a trabajar!!!\n",
        "\n",
        "##2.1 Reducción de la dimensionalidad ##\n",
        "\n",
        "Vamos a empezar entrenando un codificador automático en el conjunto de datos MNIST. Como ya conocéis de los modelos anteriores, el conjunto de datos MNIST consta de dígitos de 28 × 28 píxeles con un solo canal, lo que implica que cada dígito está representado por 28 x 28 = 784 valores. El codificador automático que vamos adesarrollar hoy  podrá comprimir esos dígitos en un vector de solo 16 valores, ¡esa es una reducción de casi el 98%!"
      ],
      "metadata": {
        "id": "5lOAx0eaAqB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos los paquetes necesarios\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, Conv2DTranspose, LeakyReLU, Activation, Flatten, Dense\n",
        "from tensorflow.keras.layers import Reshape, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2"
      ],
      "metadata": {
        "id": "28_CccrrMbw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construimos la clase donde alberageremos la estructura de autoencoder convolucional\n",
        "\n",
        "class ConvAutoencoder:\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, filters=(32, 64), latentDim=16):\n",
        "\t\t# initialize the input shape to be \"channels last\" along with\n",
        "\t\t# the channels dimension itself\n",
        "\t\t# channels dimension itself\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "    \t\t# define the input to the encoder\n",
        "\t\tinputs = ## Code ##\n",
        "\t\tx = inputs\n",
        "\t\t# loop over the number of filters\n",
        "\t\tfor f in filters:\n",
        "\t\t\t# apply a CONV => RELU => BN operation\n",
        "\t\t\tx = ## Code ##\n",
        "\t\t\tx = ## Code ##\n",
        "\t\t\tx = ## Code ##\n",
        "\t\t# flatten the network and then construct our latent vector\n",
        "\t\tvolumeSize = K.int_shape(x)\n",
        "\t\tx = ## Code ##)\n",
        "\t\tlatent = ## Code ##\n",
        "\n",
        "\t\t# Build the encoder model\n",
        "\t\tencoder = ## Code ##\n",
        "  \t\t# start building the decoder model which will accept the\n",
        "\t\t# output of the encoder as its inputs\n",
        "\t\tlatentInputs = ## Code ##\n",
        "\t\tx = ## Code ##\n",
        "\t\tx = ## Code ##\n",
        "\t\t# loop over our number of filters again, but this time in\n",
        "\t\t# reverse order\n",
        "\t\tfor f in filters[::-1]:\n",
        "\t\t\t# apply a CONV_TRANSPOSE => RELU => BN operation\n",
        "\t\t\tx = ## Code ##\n",
        "\t\t\tx = ## Code ##\n",
        "\t\t\tx = ## Code ##\n",
        "   \t\t# apply a single CONV_TRANSPOSE layer used to recover the\n",
        "\t\t# original depth of the image\n",
        "\t\tx = ## Code ##\n",
        "\t\toutputs = ## Code ##\n",
        "\n",
        "\t\t# Build the decoder model\n",
        "\t\tdecoder = ## Code ##\n",
        "\t\t# our autoencoder is the encoder + decoder\n",
        "\t\tautoencoder = ## Code ##\n",
        "\n",
        "\t\t# return a 3-tuple of the encoder, decoder, and autoencoder\n",
        "\t\treturn (encoder, decoder, autoencoder)"
      ],
      "metadata": {
        "id": "WEU2ebSaMyW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cargamos la base de datos de MNIST\n",
        "print(\"[INFO] loading MNIST dataset...\")\n",
        "((trainX, _), (testX, _)) = mnist.load_data()\n",
        "# Añadimos la dimensión relativa al canal y normalizamos las imágenes\n",
        "trainX = ## Code ##\n",
        "testX = ## Code ##\n",
        "trainX = ## Code ##\n",
        "testX = ## Code ##"
      ],
      "metadata": {
        "id": "0HsHAf-jN54s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializamos el número de épocas y bs\n",
        "EPOCHS = 25\n",
        "BS = 32\n",
        "\n",
        "#Entrenamos nuestra red Autoencoder convolucional\n",
        "print(\"[INFO] building autoencoder...\")\n",
        "(encoder, decoder, autoencoder) = C## Code ##\n",
        "opt = Adam(lr=1e-3)\n",
        "autoencoder.compile(loss=\"mse\", optimizer=opt)\n",
        "# train the convolutional autoencoder\n",
        "H = autoencoder.fit(\n",
        "\ttrainX, trainX,\n",
        "\tvalidation_data=(testX, testX),\n",
        "\tepochs=EPOCHS,\n",
        "\tbatch_size=BS)\n"
      ],
      "metadata": {
        "id": "4sQluQlMOLls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se puede ver durante el entrenamiento, nuestro proceso de entrenamiento ha sido capaz de minimizar la pérdida de reconstrucción del codificador automático. Pero, la pregunta del millón, ¿cómo de bien se han reconstruido las imágenes en base a los datos de entrenamiento?\n",
        "\n",
        "Más vale una imagen que palabras, vamos a verlo:"
      ],
      "metadata": {
        "id": "KcbmyWliPVzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "print(\"[INFO] making predictions...\")\n",
        "decoded = ## Code ##\n",
        "outputs = None\n",
        "# loop over our number of output samples\n",
        "for i in range(0, len(testX)):\n",
        "\t# grab the original image and reconstructed image\n",
        "\toriginal = ## Code ##\n",
        "\trecon = ## Code ##\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(numpy.squeeze(original, axis=-1), cmap=plt.get_cmap('gray_r'))\n",
        "plt.axis('off')\n",
        "plt.title('Imagen original')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(numpy.squeeze(recon, axis=-1), cmap=plt.get_cmap('gray_r'))\n",
        "plt.axis('off')\n",
        "plt.title('Imagen reconstruida')"
      ],
      "metadata": {
        "id": "0Hwb5vXJO52m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Como habréis podido comprobar, nuestra red ha sido capaz de recontruir las imágenes de MNIST sin ningún tipo de problema!!!!"
      ],
      "metadata": {
        "id": "0RzS6nhuPBoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##2.2 Eliminación de ruido##\n",
        "\n",
        "Una vez que hemos aprendido a reconstruir imágenes a partir de un espacio latente de una dimensión muy reducida, ahora vamos a aplicar autoencoder a otra tarea esencial en computer vision, eliminación de ruido. \n",
        "\n",
        "Los codificadores automáticos de eliminación de ruido son una extensión de los codificadores automáticos simples; sin embargo, vale la pena señalar que los codificadores automáticos de eliminación de ruido no estaban destinados originalmente a eliminar automáticamente el ruido de una imagen.\n",
        "\n",
        "\n",
        "<center> <img src=\"https://i.ibb.co/G55Y63Y/autoencoder-denoising.png\" alt=\"IA engloba ML, que a su vez engloba DL\" border=\"0\"> </center> </a>\n",
        "\n"
      ],
      "metadata": {
        "id": "DGhV3yhdQFKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos los paquetes necesarios\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, Conv2DTranspose, LeakyReLU, Activation, Flatten, Dense, Reshape\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5nUYRMcqSeJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Copiar estructura autoencoder ##"
      ],
      "metadata": {
        "id": "2CPwbhrFSkED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos la base de datos de MNIST y en este caso le añadimos ruido que luego trataremos de eliminar con nuestro modelo\n",
        "\n",
        "print(\"[INFO] loading MNIST dataset...\")\n",
        "((trainX, _), (testX, _)) = mnist.load_data()\n",
        "\n",
        "trainX = np.expand_dims(trainX, axis=-1)\n",
        "testX = np.expand_dims(testX, axis=-1)\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "\n",
        "trainNoise = ## Code ##\n",
        "testNoise = ## Code ##\n",
        "trainXNoisy = ## Code ##\n",
        "testXNoisy = ## Code ##"
      ],
      "metadata": {
        "id": "iQPKpbFdSufJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[INFO] building autoencoder...\")\n",
        "(encoder, decoder, autoencoder) = ## Code ##\n",
        "opt = Adam(lr=1e-3)\n",
        "## Code ##\n",
        "\n",
        "# train the convolutional autoencoder\n",
        "H = autoencoder.fit(\n",
        "\ttrainXNoisy, trainX,\n",
        "\tvalidation_data=(testXNoisy, testX),\n",
        "\tepochs=EPOCHS,\n",
        "\tbatch_size=BS)"
      ],
      "metadata": {
        "id": "Fe4vsTWBS-hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "print(\"[INFO] making predictions...\")\n",
        "decoded = autoencoder.predict(testX)\n",
        "outputs = None\n",
        "# loop over our number of output samples\n",
        "for i in range(0, len(testX)):\n",
        "\t# grab the original image and reconstructed image\n",
        "\toriginal = ## Code ##\n",
        "\trecon = ## Code ##\n",
        "\t# stack the original and reconstructed image side-by-side\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(numpy.squeeze(original, axis=-1), cmap=plt.get_cmap('gray_r'))\n",
        "plt.axis('off')\n",
        "plt.title('Imagen original')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(numpy.squeeze(recon, axis=-1), cmap=plt.get_cmap('gray_r'))\n",
        "plt.axis('off')\n",
        "plt.title('Imagen reconstruida')"
      ],
      "metadata": {
        "id": "XdFyDcSLTdoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Detección de anomalías"
      ],
      "metadata": {
        "id": "Yyfo-URiThuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejemplo, vamos a usar autoencoder para la detección de movimientos bancarios fraudulentos. Link a la base de datos: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
        "\n",
        "Y, ¿cómo se entrena un autoencoder para la detección de anomalías en una base de datos? La idea es que durante el entrenamiento del autoencoder, solo ingresemos transacciones normales. El bottleneck  aprenderá la representación latente de los datos de entrada normales. El decodificador utilizará la salida del bottleneck para reconstruir las transacciones normales de los datos de entrada originales.\n",
        "\n",
        "Una transacción fraudulenta será diferente de una transacción normal. El Autoencoder tendrá problemas para reconstruir la transacción fraudulenta y, por lo tanto, el error de reconstrucción será alto.\n",
        "\n",
        "Esbleceremos una transacción como fraudulenta en función de un valor de umbral especificado para el error de reconstrucción.\n",
        "\n",
        "Vamos a ello!"
      ],
      "metadata": {
        "id": "nXK4P-_FyApS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, precision_score\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "RANDOM_SEED = 2021 \n",
        "TEST_PCT = 0.3\n",
        "LABELS = [\"Normal\",\"Fraud\"]"
      ],
      "metadata": {
        "id": "51fCgXidx_5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos la base de datos\n",
        "# 1º froma si tenemos los datos subidos a google colab\n",
        "from google.colab import drive\n",
        "## Code ##\n",
        "dataset = ## Code ##"
      ],
      "metadata": {
        "id": "sxf9O_tO0kza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directamente podemos acceder a la base de datos\n",
        "!wget -O creditfraud.zip https://www.dropbox.com/s/tl20yp9bcl56oxt/creditcardfraud.zip?dl=0"
      ],
      "metadata": {
        "id": "rvYdYvb8yD9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip creditfraud.zip\n",
        "dataset = ## Code ##\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "aLbCq_IWyG_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos un análisis exploratorio de los datos\n",
        "\n",
        "print(\"Any nulls in the dataset \",dataset.isnull().values.any() )\n",
        "print('-------')\n",
        "print(\"No. of unique labels \", len(dataset['Class'].unique()))\n",
        "print(\"Label values \",dataset.Class.unique())\n",
        "#0 is for normal credit card transaction\n",
        "#1 is for fraudulent credit card transaction\n",
        "print('-------')\n",
        "print(\"Break down of the Normal and Fraud Transactions\")\n",
        "print(pd.value_counts(dataset['Class'], sort = True) )"
      ],
      "metadata": {
        "id": "Qh9_3yBY3nL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizamos los datos\n",
        "\n",
        "count_classes = ## Code ##\n",
        "count_classes.plot(kind = 'bar', rot=0)\n",
        "plt.xticks(range(len(dataset['Class'].unique())), dataset.Class.unique())\n",
        "plt.title(\"Frequency by observation number\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Number of Observations\");"
      ],
      "metadata": {
        "id": "Zxoz58EP4O_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estandarizamos las variables amount y borramos time \n",
        "dataset = ## Code ##\n",
        "sc=## Code ##\n",
        "dataset['Amount'] = ## Code ##\n",
        "\n",
        "# Dividimos los datos en train y test (recordamos que no hace falta validación son algoritmos no supervisados)\n",
        "X_train, X_test, y_train, y_test = ## Code ##"
      ],
      "metadata": {
        "id": "voKLjKSV4cu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a separar los datos por clase, de esta forma estableceremos los datos con lo que tenemos que entrenar\n",
        "X_train_normal = ## Code ##\n",
        "X_train_fraud = ## Code ##\n",
        "print(\" No. of records in Fraud Train Data=\",len(X_train_fraud))\n",
        "print(\" No. of records in Normal Train data=\",len(X_train_normal))"
      ],
      "metadata": {
        "id": "uHnGErDy6P7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecemos los datos de entrenamiento\n",
        "input_layer = ## Code ##\n",
        "encoded = ## Code ##\n",
        "decoded = ## Code ##\n",
        "autoencoder = ## Code ##"
      ],
      "metadata": {
        "id": "KmRBMsMw6myt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos el autoencoder\n",
        "## Code ##\n",
        "autoencoder.fit(X_train_normal, X_train_normal, epochs = 100, batch_size=128,\n",
        "validation_data=(X_train_normal,X_train_normal))"
      ],
      "metadata": {
        "id": "UiHilSns6q_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detección de anomalías. Las anomalías serán aquellos puntos de datos donde las pérdidas de reconstrucción son altas\n",
        "predictions = ## Code ##\n",
        "mse = ## Code ##\n",
        "error_df = pd.DataFrame({'reconstruction_error': mse,\n",
        "                        'true_class': y_train})\n",
        "error_df.groupby('true_class').describe()"
      ],
      "metadata": {
        "id": "8z79-v8N3NsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detección de anomalías. Las anomalías serán aquellos puntos de datos donde las pérdidas de reconstrucción son altas\n",
        "test_x_predictions = autoencoder.predict(X_test)\n",
        "mse = ## Code ##\n",
        "error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
        "                        'True_class': y_test})\n",
        "y_pred=## Code ##"
      ],
      "metadata": {
        "id": "FWRs9c1_8xom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ploteamos los puntos de test y su respectivo error de reconstrucción y establecemos un valor a partir del cuán el punto se considera anómalo.\n",
        "threshold_fixed = 11\n",
        "groups = error_df.groupby('True_class')\n",
        "fig, ax = plt.subplots()\n",
        "for name, group in groups:\n",
        "    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
        "            label= \"Fraud\" if name == 1 else \"Normal\")\n",
        "ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
        "ax.legend()\n",
        "plt.title(\"Reconstruction error for normal and fraud data\")\n",
        "plt.ylabel(\"Reconstruction error\")\n",
        "plt.xlabel(\"Data point index\")\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "kBXUu4Xp98QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluamos la performance de nuestro modelo\n",
        "from sklearn import metrics\n",
        "import matplotlib.ticker as ticker\n",
        "conf_matrix = metrics.confusion_matrix(y_test,y_pred)\n",
        "\n",
        "ax=plt.subplot()\n",
        "sns.heatmap(conf_matrix,annot=True,ax=ax,fmt='g')#annot=True to annotate cells, fmt='g' numbers not scientific form\n",
        "ax.set_xlabel('Predicted labels'); ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(['Normal', 'Fraud']); ax.yaxis.set_ticklabels(['Normal', 'Fraud']);\n",
        "ax.set(yticks=[0, 2], \n",
        "       xticks=[0.5, 1.5])\n",
        "ax.yaxis.set_major_locator(ticker.IndexLocator(base=1, offset=0.5))"
      ],
      "metadata": {
        "id": "ZyNfpwoB-9gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Variarional Autoencoder**\n",
        "\n",
        "El Autoencoder variacional es ligeramente diferente al autoencoder clásico. En lugar de aprender directamente de las características latentes de las muestras de entrada, aprende de su distribución. Se supone que las características latentes de los datos de entrada siguen una distribución normal estándar. Esto significa que se supone que los vectores latentes aprendidos están centrados en cero y se pueden representar con dos estadísticas: media y varianza (ya que la distribución normal estándar se puede atribuir solo con estas dos estadísticas).\n",
        "Por lo tanto, los autoencoders variacionales (VAE) calculan la media y la varianza de los vectores latentes (en lugar de aprender directamente las características latentes) para cada muestra y las obligan a seguir una distribución normal estándar. Por lo tanto, el bottleneck de la red se utiliza para conocer la media y la varianza de cada muestra. Definiremos dos capas diferentes totalmente conectadas (FC) para calcular ambas. Los VAE aseguran que los puntos que están muy cerca uno del otro en el espacio latente representan muestras de datos muy similares (clases de datos similares). \n",
        "\n",
        "<center> <img src=\"https://i.ibb.co/g4XLjTx/VAE.png\" alt=\"Arquiitectura VAE\" border=\"0\"> </center> </a>\n",
        "\n",
        "Antes de saltar a los detalles de implementación, primero entendamos un poco la divergencia Kullback-Leibler (KL) que se utilizará como una de las dos medidas de optimización en nuestro modelo.\n",
        "\n",
        "\n",
        "**Divergencia Kullback-Leibler (KL)**\n",
        "\n",
        "Como hemos comentado, la principal diferencia entre un autoencoder convencional y variational autoencoder reside en que este último fuerza a que las características del estado latente cumplan una distribución normal estándar. Esto se puede lograr utilizando estadísticas de divergencia KL. KL-divergencia es una medida estadística de la diferencia entre dos distribuciones probabilísticas. Por lo tanto, utilizaremos el valor de divergencia KL como una función objetivo (junto con la pérdida de reconstrucción que ya utilizabamos en el autoencoder convencional) para garantizar que la distribución aprendida sea muy similar a la distribución real, que ya hemos asumido como una distribución normal estándar.\n",
        "\n",
        "Por tanto, las pérdidas de nuestro modelo vendrán dadas por:\n",
        "\n",
        "\n",
        "<center> <img src=\"https://i.ibb.co/YQg5SpG/VAE-error.png\"  </center> </a>\n",
        "\n"
      ],
      "metadata": {
        "id": "PCtKd2lqeBaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "Se1FHa0WCfhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos la base de datos de MNIST\n",
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "# En este caso, juntamos los datos de entrenamiento y test porque vamos a realizar\n",
        "# la tarea de generación de imágenes\n",
        "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
        "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255"
      ],
      "metadata": {
        "id": "01MAFO1eC4zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Genereamos la capa de sampling (final de la parte de autoencer)\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = ## Code ##\n",
        "        dim = ## Code ##\n",
        "        epsilon = ## Code ##\n",
        "        return ## Code ##"
      ],
      "metadata": {
        "id": "OlfN1bo0DZhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la parte del encoder\n",
        "latent_dim = 2\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "z_mean = ## Code ##\n",
        "z_log_var = ## Code ##\n",
        "z = ## Code ##\n",
        "encoder = ## Code ##\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "aRoKT9orDl5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la parte de decoder\n",
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((7, 7, 64))(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = ## Code ##\n",
        "decoder.summary()"
      ],
      "metadata": {
        "id": "B1guZN6ADqxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el entrenamiento de una red. En este caso, haremos uso de gradientTape.\n",
        "# Esta función se utiliza cuando el aprendizaje es más complejo de realizar\n",
        "# En este caso, podemos definirnos nuestro propio entrenamiento\n",
        "\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = ## Code ##\n",
        "            reconstruction = s## Code ##\n",
        "            reconstruction_loss = ## Code ##\n",
        "\n",
        "            kl_loss =## Code ##\n",
        "            kl_loss = ## Code ##\n",
        "            total_loss = ## Code ##\n",
        "        grads = ## Code ##\n",
        "        ## Code ##\n",
        "        ## Code ##\n",
        "        ## Code ##\n",
        "        ## Code ##\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "RQGFWzJvD6xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos el modelo VAE, compilamos y entrenamos el modelo\n",
        "vae = ## Code ##\n",
        "## Code ##\n",
        "vae.fit(mnist_digits, epochs=30, batch_size=128)"
      ],
      "metadata": {
        "id": "8kNY6IGUEUEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ploteamos ahora los ejemplos generados\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_latent_space(vae, n=30, figsize=15):\n",
        "    # display a n*n 2D manifold of digits\n",
        "    digit_size = 28\n",
        "    scale = 1.0\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of digit classes in the latent space\n",
        "    grid_x = np.linspace(-scale, scale, n)\n",
        "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = vae.decoder.predict(z_sample)\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "            figure[\n",
        "                i * digit_size : (i + 1) * digit_size,\n",
        "                j * digit_size : (j + 1) * digit_size,\n",
        "            ] = digit\n",
        "\n",
        "    plt.figure(figsize=(figsize, figsize))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = n * digit_size + start_range\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure, cmap=\"Greys_r\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_latent_space(vae)"
      ],
      "metadata": {
        "id": "HOaSvwVjEoFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostramos como podemos diferenciar en cluster el espacio latente de los diferentes digitos\n",
        "def plot_label_clusters(vae, data, labels):\n",
        "    # display a 2D plot of the digit classes in the latent space\n",
        "    z_mean, _, _ = ## Code ##\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
        "\n",
        "plot_label_clusters(vae, x_train, y_train)\n"
      ],
      "metadata": {
        "id": "mTMiEncbEsYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Generative Adversarial Neural Network (GANs)**\n",
        "\n",
        "Las GANs son un tipo muy especial de redes neuronales que se componen de 2 partes: un generador y un discriminador.\n",
        "\n",
        "Con el siguiente esquema entenderéis mejor cómo funcionan:\n",
        "\n",
        "\n",
        "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*N3nT9AXVnsFBta2R1eEMjg.png\"></center>\n",
        "\n",
        "Y aqui el esquema típico que veréis siempre:\n",
        "\n",
        "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*N4oqJsGmH-KZg3Vqrm_uYw.jpeg\"></center>\n",
        "\n",
        "\n",
        "\n",
        "Y os preguntaréis, ¿en qué se diferencia un Autoencoder de una GAN?\n",
        "\n",
        "Tanto las GANs como los autoencoders son modelos generativos. Sin embargo, un autoencoder esencialmente está aprendiendo una función de identidad mediante la compresión de los datos. Como ya hemos visto, el autoencoder comprimirá nuestros datos de entrada a un espacio latente de menor dimensión. Posteriormente, intentará reconstruir la entrada usando solo el vector del espacio latente.\n",
        "\n",
        "Sin embargo, la GAN tiene las siguientes características:\n",
        "\n",
        "* Acepta una entrada de baja dimensión.\n",
        "* Construye un espacio de alta dimensión a partir de él.\n",
        "* Genera el resultado final, que no forma parte de los datos de entrenamiento originales, pero idealmente pasa como tal.\n",
        "\n",
        "A medida que se entrena una GAN, el modelo generativo genera imágenes \"falsas\" que luego se mezclan con imágenes \"reales\". El modelo discriminador debe determinar qué imágenes son \"reales\" frente a \"falsas/generadas\".\n",
        "\n",
        "A medida que el modelo generativo se vuelve cada vez mejor en la generación de imágenes falsas que pueden engañar al discriminador, el término de pérdidas evoluciona y cambia (esta es una de las razones por las que entrenar GAN es tan difícil).\n",
        "\n",
        "Los autoencoders no pueden generar nuevos datos realistas que los humanos podrían considerar \"pasables\". Por ello, para la síntesis de nuevos datos, vuestros mejores amigos serán las GANs. A continuación, vamos a desarrollar un ejemplo de síntesis de imágenes:"
      ],
      "metadata": {
        "id": "oxNJ89mfisQZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu4aL3teCMPW"
      },
      "source": [
        "'''\n",
        "DCGAN on MNIST using Keras\n",
        "Author: Rowel Atienza\n",
        "Project: https://github.com/roatienza/Deep-Learning-Experiments\n",
        "Usage: python3 dcgan_mnist.py\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "# tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Reshape\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
        "from tensorflow.keras.layers import LeakyReLU, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ElapsedTimer(object):\n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "    def elapsed(self,sec):\n",
        "        if sec < 60:\n",
        "            return str(sec) + \" sec\"\n",
        "        elif sec < (60 * 60):\n",
        "            return str(sec / 60) + \" min\"\n",
        "        else:\n",
        "            return str(sec / (60 * 60)) + \" hr\"\n",
        "    def elapsed_time(self):\n",
        "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )\n",
        "\n",
        "class DCGAN(object):\n",
        "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
        "\n",
        "        self.img_rows = img_rows\n",
        "        self.img_cols = img_cols\n",
        "        self.channel = channel\n",
        "        self.D = None   # discriminator\n",
        "        self.G = None   # generator\n",
        "        self.AM = None  # adversarial model\n",
        "        self.DM = None  # discriminator model\n",
        "\n",
        "    # (W−F+2P)/S+1\n",
        "    def discriminator(self):\n",
        "        if self.D:\n",
        "            return self.D\n",
        "        self.D = Sequential()\n",
        "        depth = 64\n",
        "        dropout = 0.4\n",
        "        # In: 28 x 28 x 1, depth = 1\n",
        "        # Out: 14 x 14 x 1, depth=64\n",
        "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
        "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\n",
        "            padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        # Out: 1-dim probability\n",
        "        self.D.add(Flatten())\n",
        "        self.D.add(Dense(1))\n",
        "        self.D.add(Activation('sigmoid'))\n",
        "        self.D.summary()\n",
        "        return self.D\n",
        "\n",
        "    def generator(self):\n",
        "        if self.G:\n",
        "            return self.G\n",
        "        self.G = Sequential()\n",
        "        dropout = 0.4\n",
        "        depth = 64+64+64+64\n",
        "        dim = 7\n",
        "        # In: 100\n",
        "        # Out: dim x dim x depth\n",
        "        self.G.add(Dense(dim*dim*depth, input_dim=100))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "        self.G.add(Reshape((dim, dim, depth)))\n",
        "        self.G.add(Dropout(dropout))\n",
        "\n",
        "        # In: dim x dim x depth\n",
        "        # Out: 2*dim x 2*dim x depth/2\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
        "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
        "        self.G.add(Activation('sigmoid'))\n",
        "        self.G.summary()\n",
        "        return self.G\n",
        "\n",
        "    def discriminator_model(self):\n",
        "        if self.DM:\n",
        "            return self.DM\n",
        "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
        "        self.DM = Sequential()\n",
        "        self.DM.add(self.discriminator())\n",
        "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
        "            metrics=['accuracy'])\n",
        "        return self.DM\n",
        "\n",
        "    def adversarial_model(self):\n",
        "        if self.AM:\n",
        "            return self.AM\n",
        "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
        "        self.AM = Sequential()\n",
        "        self.AM.add(self.generator())\n",
        "        self.AM.add(self.discriminator())\n",
        "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
        "            metrics=['accuracy'])\n",
        "        return self.AM\n",
        "\n",
        "class MNIST_DCGAN(object):\n",
        "    def __init__(self):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channel = 1\n",
        "\n",
        "        #self.x_train = input_data.read_data_sets(\"mnist\",\\\n",
        "        \t#one_hot=True).train.images\n",
        "        (self.x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "        self.x_train = self.x_train.reshape(-1, self.img_rows,\\\n",
        "        \tself.img_cols, 1).astype(np.float32)\n",
        "\n",
        "        self.DCGAN = DCGAN()\n",
        "        self.discriminator =  self.DCGAN.discriminator_model()\n",
        "        self.adversarial = self.DCGAN.adversarial_model()\n",
        "        self.generator = self.DCGAN.generator()\n",
        "\n",
        "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
        "        noise_input = None\n",
        "        if save_interval>0:\n",
        "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
        "        for i in range(train_steps):\n",
        "            images_train = self.x_train[np.random.randint(0,\n",
        "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            images_fake = self.generator.predict(noise)\n",
        "            x = np.concatenate((images_train, images_fake))\n",
        "            y = np.ones([2*batch_size, 1])\n",
        "            y[batch_size:, :] = 0\n",
        "            d_loss = self.discriminator.train_on_batch(x, y)\n",
        "\n",
        "            y = np.ones([batch_size, 1])\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
        "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
        "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
        "            print(log_mesg)\n",
        "            if save_interval>0:\n",
        "                if (i+1)%save_interval==0:\n",
        "                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n",
        "                        noise=noise_input, step=(i+1))\n",
        "\n",
        "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
        "        filename = 'mnist.png'\n",
        "        if fake:\n",
        "            if noise is None:\n",
        "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
        "            else:\n",
        "                filename = \"mnist_%d.png\" % step\n",
        "            images = self.generator.predict(noise)\n",
        "        else:\n",
        "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
        "            images = self.x_train[i, :, :, :]\n",
        "\n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(images.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            image = images[i, :, :, :]\n",
        "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
        "            plt.imshow(image, cmap='gray')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        if save2file:\n",
        "            plt.savefig(filename)\n",
        "            plt.close('all')\n",
        "        else:\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJdc8z0ICZBw"
      },
      "source": [
        "mnist_dcgan = MNIST_DCGAN()\n",
        "timer = ElapsedTimer()\n",
        "mnist_dcgan.train(train_steps=10000, batch_size=256, save_interval=500)\n",
        "timer.elapsed_time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCN7sdFVC3qN"
      },
      "source": [
        "mnist_dcgan.plot_images(fake=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KNFMn_juqkZ"
      },
      "source": [
        "mnist_dcgan.plot_images(fake=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y hasta aquí el módulo de deep learning. Como podréis observar hemos hecho un gran recorrido por el deep learning, empezando con los aspetos más básicos hasta acabar con la implmentación de modelos genereativos. Espero que hayáis aprendido mucho!"
      ],
      "metadata": {
        "id": "46e75tLWOoEW"
      }
    }
  ]
}