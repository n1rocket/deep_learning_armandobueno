{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OFDhMkz78NoB",
        "oLJb8m8c8TSH"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJQZDOSRlvjY"
      },
      "source": [
        "# **Módulo 1. Deep Learning y Tensorflow**\n",
        "\n",
        "¡Bienvenidos al apasionante mundo del Deep Learning! \n",
        "\n",
        "En este primer módulo, el alumno conocerá los princiaples fundamentos del area del deep learning y se familiarizará con tensorflow. Para ello, comenzaremos con una pequeña introducción al deep learning y su diferencia con inteligencia artificial y machine learning, repsaremos su contexto histórico, aprenderemos los principales tipos de aprendizajes exsistenes e introduciremos google colab.\n",
        "\n",
        "Posteriormente, comenzaremos a trabajar con TensorFlow, y al terminar la sesión, ¡habréis implementado vuestra **primera red neuronal**!\n",
        "\n",
        "Aquí tenéis el temario que cubrirá este módulo:\n",
        "\n",
        "\n",
        " **1.** **Introducción al Deep learning**\n",
        "  <ul>\n",
        " 1.1 Del Machine Learning al Deep Learning\n",
        "\n",
        "  1.2 Contexto histórico\n",
        "\n",
        "  1.3 Tipos de aprendizaje\n",
        "\n",
        "  1.4 Introducción a Google Colab\n",
        "  </ul>\n",
        "\n",
        "**2.** **Introducción a TensorFlow**\n",
        "  <ul>\n",
        "  2.1 Definición de tensores\n",
        "\n",
        "  2.2 Sesiones iterativas y grafos\n",
        "\n",
        "  2.3 Problemas de optimización. Descenso de gradiente\n",
        "  <ul>\n",
        "  2.3.1 Ejercicio 1\n",
        "\n",
        "  2.3.3 Ejercicio 2\n",
        "  \n",
        "  2.3.3 Ejercicio 3. Regresión Lineal\n",
        "\n",
        "  2.3.4 Ejercicio 4. Regresión Logística\n",
        "  </ul>\n",
        "  </ul>\n",
        "\n",
        "\n",
        "**¡Vamos allá!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0dQrH-TDD1z"
      },
      "source": [
        "# 1. Introducción al Deep Learning\n",
        "\n",
        "## 1.1. Del Machine Learning al Deep Learning\n",
        "\n",
        "¿Qué es el Deep Learning? ¿En qué se diferencia del Machine Learning? ¿Y de la Inteligencia Artificial?\n",
        "\n",
        "<center> <a href=\"https://ibb.co/hWqjG7\"><img src=\"https://preview.ibb.co/cETFOn/AI_ML_DL.png\" alt=\"IA engloba ML, que a su vez engloba DL\" border=\"0\"> </center> </a>\n",
        "\n",
        "**Inteligencia artificial:** \n",
        "\n",
        "Inteligencia Artificial (IA) es un conjunto de algoritmos y técnicas que pueden ser\n",
        "usadas para resolver problemas que los humanos realizamos intuitivamente, pero\n",
        "que son realmente difíciles para un ordenador. Se puede dividir en dos campos:\n",
        "\n",
        "*   *General AI*: consiste en dotar a las máquinas de todas nuestras capacidades y sentidos. Por ejemplo, C-3PO o Terminator. Tranquilos, todavía queda muuuucho para ver algo parecido a Skynet por aquí.\n",
        "*   *Narrow AI*: consiste en dotar a las máquinas de la capacidad de desarrollar una determinada tarea, como por ejemplo, reconocer caras, señales de tráfico, el habla, etc. Es en este campo donde actualmente se están observando grandes avances.\n",
        "\n",
        "**Machine Learning:**\n",
        "\n",
        "El Machine Learning (ML) o aprendizaje máquina es un **subcampo dentro de la Inteligencia Artificial**. Básicamente, consiste en **utilizar una gran cantidad de datos para extraer información útil para las personas**. Por ejemplo, imaginaos que disponemos de los últimos resultados de La Liga y queremos ser capaces de predecir el resultado del próximo Clásico. Primero, necesitariamos *parsear* los datos, limpiarlos, eliminar las entradas incompletas, estudiar la distribución de las variables y elegir las características o atributos (*feature engineering*) que nos permitieran predecir este resultado con la mayor precisión posible. Una vez tuviésemos claro cuales son las mejores características a utilizar, necesitaríamos encontrar el mejor algoritmo posible para nuestro dataset.\n",
        "\n",
        "La **elección de las características**, que a priori puede llegar a parecer sencillo, es el paso **más complicado y costoso** y, además, requiere de un alto grado de conocimientos sobre el problema en cuestión y sobre técnicas de extracción de características.\n",
        "\n",
        "<center> <a  <href=\"https://ibb.co/3s04DyH\"><img src=\"https://i.ibb.co/MVpvSx2/machine-learning.jpg\" alt=\"IA engloba ML, que a su vez engloba DL\" align= \"middle\" border=\"1\"> </center></a>\n",
        "\n",
        "\n",
        "**Deep Learning:**\n",
        "\n",
        "El aprendizaje profundo o Deep Learning (DL) es un subcampo del **Machine Learning**, y soluciona el problema anterior de la elección de características. También llamado aprendizaje jerárquico, ***aprende*** distintas **representaciones de los datos** que son introducidas a un clasificador final.\n",
        "\n",
        "La magia está en que ya no necesitamos volvernos locos buscando las mejores características o atributos para cada problema, si no que esto lo hace **automáticamente** nuestro algoritmo.\n",
        "\n",
        "<center> <a  <href=\"https://ibb.co/3s04DyH\"><img src=\"https://i.ibb.co/zbjHVnW/deep-learning.jpg\" alt=\"Deep learning\"  border=\"1\"> </center></a>\n",
        "\n",
        "Por último, cabe destacar la importancia de tener una gran cantidad de datos para poder utilizar técnicas de ML o DL. Además, cuanta más calidad tengan esos datos, mejor se comportaran nuestros modelos.\n",
        "\n",
        "*“I think AI is akin to building a rocket ship. You need a huge engine and a lot of fuel. If you have a large engine and a tiny amount of fuel, you won’t make it to orbit. If you have a tiny engine and a ton of fuel, you can’t even lift off. To build a rocket you need a huge engine and a lot of fuel.”*\n",
        "\n",
        "– Andrew Ng (source: Wired)\n",
        "\n",
        "Que traducido quiere decir:\n",
        "\n",
        "*Creo que la IA es parecido a construir un cohete espacial. Necesitas un motor enorme y mucho combustible. Si tienes un motor enorme pero poco combustible, no conseguirás poner el cohete en órbita. Si tienes un pequeño motor y un montón de combustible, no podrás ni despegar. Para construir un cohete espacial necesitas un motor enorme y un monton de combustible.*\n",
        "\n",
        "Así que ya sabéis, no solo es importante el algoritmo que utilicemos, sino los datos de los que dispongamos.\n",
        "\n",
        "\n",
        "\n",
        "<center> <img src=\"https://i.ibb.co/44fMMTF/machine-learning-vs-deep.jpg\" border=\"0\" height=\"250\"  height=\"300\" align= \"middle\"> </center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFDhMkz78NoB"
      },
      "source": [
        "## 1.2. Contexto histórico\n",
        "\n",
        "Probablemente muchos de vosotros penséis que el área del Deep Learning y las redes neuronales es una novedad, pues, ¡nada más lejos de la realidad! De hecho, **a primera red neuronal data de 1958**. El problema es que no había potencia de cálculo suficiente para poder progresar hasta hace relativamente poco, con la popularización de las **GPUs** de altas prestaciones. Así que... gracias gamers!!\n",
        "\n",
        "Por si tenéis curiosidad, estos son los hechos más importantes que han ocurrido en el campo de la IA:\n",
        "\n",
        "<img src=\"https://image.ibb.co/mEVbA8/timeline_deep_learning.png\" alt=\"timeline_deep_learning\" border=\"0\">\n",
        "\n",
        "*   **1950**: Alan Turing crea el **Test de Turing**\n",
        "*   **1952**: Arthur Samuel crea el primer programa que **aprende** partida tras partida a jugar a las Damas\n",
        "*   **1956**: Martin Minsky acuña el término **\"Artificial Intelligence\"** para referirse a este nuevo campo\n",
        "*   **1958**: Frank Rosenblatt diseña el Perceptrón, **la primera red neuronal artificial**. HACE **60 AÑOS!!!**\n",
        "\n",
        "Como podéis ver, la Inteligencia Artificial no es ni mucho menos nueva, lleva mucho tiempo entre nosotros, sufriendo altibajos debidos a las altas expectativas depositadas y los \"pocos\" avances conseguidos.\n",
        "\n",
        "Desde 1974 hasta nuestros días, se han producido varios \"inviernos\" y \"primaveras\".\n",
        "\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://image.ibb.co/b3ih3n/AI_winter.jpg\" alt=\"Winter is coming\" border=\"0\"></a>\n",
        "\n",
        "Algunos de los hechos más relevantes hasta 2006 son estos:\n",
        "\n",
        "*   **1967**: Nace el campo del reconocimiento de patrones con la aparición del algoritmo **\"Nearest Neighbor\"**\n",
        "*   **1979**: Crean el Stanford Cart, un **robot capaz de navegar automáticamente por una habitación evitando obstáculos**\n",
        "*   **1981**: Gerald Dejong introduce el \"Explanation Based Learning\", el precursor del Machine Learning. El ordenador **analizaba datos** de entrenamiento y creaba reglas **para descartar los datos menos importantes**\n",
        "*   **1985**: Terry Sejnowski inventa NetTalk, un algoritmo capaz de **aprender a pronunciar palabras** como lo haría un niño\n",
        "*   **1990s**: **Cambia el paradigma del Machine Learning**: de un enfoque orientado al conocimiento, a uno **orientado al dato**. Empiezan a extraer conclusiones de grandes cantidades de datos.\n",
        "*   **1997**: **DeepBlue derrota a Kaspárov** por primera vez.\n",
        "\n",
        "Tras esta época de luces y sombras, y gracias, entre otras cosas, a la disponibilidad de mucha más potencia de cálculo y de datos, desde 2006 hasta la fecha ha habido una explosión del Machine Learning.\n",
        "\n",
        "*   **2006**: Aparecen las **arquitecturas profundas**, acuñadas como Deep Learning por Geoffrey Hinton\n",
        "*   **2011**: **Watson IBM vence** a sus competidores **humanos** en el concurso Jeopardy\n",
        "*   **2012**: Geoffrey Hinton gana por goleada el concurso de **ImageNet** con una red neuronal profunda\n",
        "*   **2012**: En Google X crean GoogleBrain, capaz de **detectar gatos en videos**\n",
        "*   **2014**: Facebook desarrolla DeepFace, capaz de **reconocer caras de humanos con una precisión de 97.25%**, solo un 0.28% por debajo de un ser humano\n",
        "*   **2014**: Google compra DeepMind, una stertup inglesa que había creado un algoritmo capaz de **aprender a jugar con juegos Atari simplemente viendo el video**\n",
        "*   **2014**: Aparecen las **GANs (Generative Adversarial Networks)**, capaces de generar contenido falso que no lo parece, cambiando para siempre la percepción de \"real\" (deep fakes).\n",
        "*   **2015**: Elon Musk y Sam Altman crean OpenAI para **promover el bueno uso de la IA**\n",
        "*   **2016**: **Google DeepMind vence** por primera vez en el **juego Go, consiguiendo movimientos creativos**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Tipos de aprendizaje\n",
        "\n",
        "Como con cualquier método, existen diferentes formas de entrenar los algoritmos de aprendizaje profundo, cada una con sus propias ventajas y desventajas. Para comprender los pros y los contras de cada tipo de aprendizaje automático, primero debemos observar cuáles son sus inputs. En deep learning, hay dos tipos de datos: datos etiquetados y datos no etiquetados.\n",
        "\n",
        "Los datos etiquetados tienen los parámetros de entrada y salida en un patrón completamente legible por máquina, pero requiere mucho trabajo humano para etiquetar los datos. Los datos sin etiquetar solo tienen uno o ninguno de los parámetros en un formato legible por máquina. Esto reduce la necesidad de mano de obra humana pero requiere soluciones más complejas. Podemos dividir los tipos de aprendizajes en deep learning en:\n",
        "\n",
        "**Supervised Learning**\n",
        "\n",
        "El aprendizaje supervisado es el paradigma más popular para el aprendizaje automático. Dados los datos en forma de ejemplos con etiquetas, podemos alimentar un algoritmo de aprendizaje con estos pares de etiqueta-ejemplo, lo que permite que el algoritmo prediga la etiqueta para cada ejemplo y le proporcione retroalimentación sobre si predijo la respuesta correcta o no. Con el tiempo, el algoritmo aprenderá a aproximarse a la naturaleza exacta de la relación entre los ejemplos y sus etiquetas. Cuando esté completamente capacitado, el algoritmo de aprendizaje supervisado podrá observar un nuevo ejemplo nunca antes visto y predecir una buena etiqueta para él.\n",
        "\n",
        "<center> <img src=\"https://i.ibb.co/Jp9KF00/aprendizaje-supervisado.png\" alt=\"IA engloba ML, que a su vez engloba DL\" border=\"0\"  width=\"700\" \n",
        "     height=\"400\"> </center> </a>\n",
        "\n",
        "\n",
        "\n",
        "**Unsupervised Learning**\n",
        "\n",
        "El aprendizaje no supervisado es opuesto al aprendizaje supervisado. No presenta etiquetas. En cambio, nuestro algoritmo recibiría una gran cantidad de datos y se le darían las herramientas para comprender las propiedades de los datos. A partir de ahí, puede aprender a agrupar, agrupar y/u organizar los datos de tal manera que un ser humano (u otro algoritmo inteligente) pueda ingresar y dar sentido a los datos recién organizados.\n",
        "\n",
        "<center> <img src=\"https://i.ibb.co/RgtVwHp/aprendizaje-no-supervisado-1.png\" alt=\"IA engloba ML, que a su vez engloba DL\" border=\"0\"  width=\"800\" \n",
        "     height=\"400\"> </center> </a>\n",
        "\n",
        "Dentro del aprendizaje no supervisado destacan los algoritmos generativos.\n",
        "\n",
        "**Reinforcement Learning**\n",
        "\n",
        "El aprendizaje por refuerzo es bastante diferente en comparación con el aprendizaje supervisado y no supervisado. Si bien podemos ver fácilmente la relación entre supervisado y no supervisado (la presencia o ausencia de etiquetas), la relación con el aprendizaje por refuerzo es un poco más complicada. El aprendizaje por refuerzo básicamente consiste en aprender de los errores. Si colocamos un algoritmo de aprendizaje por refuerzo en cualquier entorno, al principio cometerá muchos errores. Siempre que proporcionemos algún tipo de señal al algoritmo que asocie los buenos comportamientos con una señal positiva y los malos comportamientos con una negativa, podemos reforzar nuestro algoritmo para preferir los buenos comportamientos a los malos. Con el tiempo, nuestro algoritmo de aprendizaje aprenderá a cometer menos errores que antes. Para cualquier problema de aprendizaje por refuerzo, necesitamos un agente y un entorno, así como una forma de conectar los dos a través de un circuito de retroalimentación. Para conectar al agente con el entorno, le damos un conjunto de acciones que puede realizar y que afectan al entorno. Para conectar el entorno con el agente, hacemos que emita continuamente dos señales al agente: un estado actualizado y una recompensa (nuestra señal de refuerzo para el comportamiento).\n",
        "\n",
        "<center> <img src=\"https://i.ibb.co/c2qvTjn/aprendizaje-por-refuerzo.png\" alt=\"IA engloba ML, que a su vez engloba DL\" border=\"0\"  width=\"900\" \n",
        "     height=\"400\"> </center> </a>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MsJ9YYHPgLFp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLJb8m8c8TSH"
      },
      "source": [
        "## 1.4. Introducción a Google Colab\n",
        "\n",
        "La plataforma en la que os encontráis se llama **Google Colab**. Va a ser a la vez nuestro *notebook* y nuestra IDE, ya que nos permite aprovechar la GPU **NVIDIA K80** que por el momento ofrece Google de forma **gratuita**.\n",
        "\n",
        "Cualquiera con una cuenta de Google puede crear un nuevo notebook y empezar a disfrutarlo! A lo largo del curso iremos viendo lo necesario para que aprendáis a manejarlo con soltura, así que no tenéis por qué preocuparos si es la primera vez que lo véis. Y si habéis trabajado antes con Jupyter, ¡esto es lo mismo!\n",
        "\n",
        "Lo único que tenéis que hacer todos para aprovechar la GPU es ir a **Edit**, en la barra de herramientas, y luego a **Notebook settings**. Os aparecerá esta pantalla, la cual tiene que quedar igual que en la imagen. Debéis seleccionar Python 3 y GPU, y luego darle a **Save**.\n",
        "\n",
        "<a href=\"https://ibb.co/ceoX3n\"><img src=\"https://preview.ibb.co/mQEi9S/notebook_config.png\" alt=\"Notebook configuration\" border=\"0\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcokI_bpc4Ui"
      },
      "source": [
        "## 2. Introducción a TensorFlow\n",
        "\n",
        "Como muchos de vosotros ya sabréis, TensorFlow es un framework desarrollado y mantenido por Google que permite la ejecución de operaciones matemáticas de una forma optimizada en una CPU o GPU. En nuestro caso estamos más interesados en la GPU, ya que es la única forma que tenemos de entrenar una red neuronal profunda y no morir de viejos esperando ;)\n",
        "\n",
        "¿Por qué tensorflow?\n",
        "\n",
        "* Por su **flexibilidad y escalabilidad**\n",
        "\n",
        "* Por su **popularidad**\n",
        "\n",
        "<img src=\"https://image.ibb.co/mCJNqd/tf_pop.png\" alt=\"tf_pop\" border=\"0\" height=\"300\"> <img src=\"https://image.ibb.co/bBDYwJ/tf_users.png\" alt=\"tf_users\" border=\"0\" height=\"300\">\n",
        "\n",
        "\n",
        "Más adelante comprenderemos por qué es tan importante disponer de un buen framework que nos permita realizar operaciones de la forma más rápida posible. De momento, vamos a trabajar un poco con TensorFlow hasta que nos familiaricemos con su modo de funcionar. Pero antes, algunas de sus características más importantes:\n",
        "\n",
        "*    TensorFlow utiliza **tensores** para realizar las operaciones.\n",
        "*    En TensorFlow, primero se definen las operaciones a realizar (construimos el **grafo**), y luego se ejecutan (se ejecuta el grafo)\n",
        "*    Permite ejecutar el código implementado **paralelamente** o en una o varias GPUs, a elección del usuario\n",
        "\n",
        "Vale, pero **¿qué es un tensor?**\n",
        "\n",
        "## 2.1 Definición de tensores\n",
        "Aunque los tensores los inventaron los físicos para ser capaces de describir interacciones, en el ámbito de la IA se pueden entender simplemente como **contenedores de números**.\n",
        "\n",
        "<a href=\"https://ibb.co/bBCyb7\"><img src=\"https://image.ibb.co/fnFvOn/tensores.png\" alt=\"tensores\" border=\"0\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¡Vamos a practicar con nuestros nuevos mejores amigos, los Tensores!"
      ],
      "metadata": {
        "id": "9_7r3PgCqTMQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIIqtgJf3P_5"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Imaginaos que queréis guardar la nota media del máster de Tristina Cipuentes.\n",
        "# Para ello utilizaríais un tensor de 0D, que no es otra cosa que un simple \n",
        "# número, también conocido como escalar.\n",
        "# Definición del tensor 0D\n",
        "arr0d=np.array(5)\n",
        "tensor_0D=tf.convert_to_tensor(arr0d, tf.float64)\n",
        "# De este escalar, podemos ver tanto su contenido como ciertas propiedades\n",
        "print(\"Nota media:{}\".format(arr0d))\n",
        "print(\"Dimensiones del tensor: {}\".format(tensor_0D.ndim))\n",
        "print(\"Tamaño  del tensor: {}\".format(tensor_0D.shape))\n",
        "print(\"Tipo  del tensor: {}\".format(tensor_0D.dtype))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkk4unVy4CwS"
      },
      "source": [
        "# Pero con una sola nota no habría mucho que defender, podría decirse incluso\n",
        "# que alguien se la ha inventado, así que mejor si guardamos las notas de\n",
        "# TODOS las asignaturas que hizo. Podemos usar un tensor 1D para ello:\n",
        "\n",
        "# Tensor 1D (vector)\n",
        "array_1D=np.array([2,6,8])\n",
        "tensor_1D=tf.convert_to_tensor(array_1D, tf.float64)\n",
        "\n",
        "print(\"Notas de las asignaturas: {}\".format(tensor_1D))\n",
        "print(\"Dimensiones del tensor: {}\".format(tensor_1D.ndim))\n",
        "print(\"Tamaño  del tensor: {}\".format(tensor_1D.shape))\n",
        "print(\"Tipo  del tensor: {}\".format(tensor_1D.dtype))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ab-ex3W4tfc"
      },
      "source": [
        "# Un momento... ¿no sería mejor que estructurásemos las notas por asignatura?\n",
        "# ¿Cómo podríamos hacerlo, si cada asignatura consta de 3 exámenes? \n",
        "# ¿Se os ocurre alguna estructura que nos lo permita?\n",
        "\n",
        "# En efecto, ¡un tensor 2D! ¡ Una matriz de números!\n",
        "\n",
        "# Tensor 2D (matriz)\n",
        "## Code ##\n",
        "## Code ##\n",
        "\n",
        "print(\"Las puntuaciones de Tristina en sus exámenes son:\\n{}\".format(tensor_2D))\n",
        "print(\"Asignatura 1:\\n{}\".format(tensor_2D[0]))\n",
        "print(\"Asignatura 2:\\n{}\".format(tensor_2D[1]))\n",
        "print(\"Asignatura 3:\\n{}\".format(tensor_2D[2]))\n",
        "print(\"Dimensiones del tensor: {}\".format(tensor_2D.ndim))\n",
        "print(\"Tamaño  del tensor: {}\".format(tensor_2D.shape))\n",
        "print(\"Tipo  del tensor: {}\".format(tensor_2D.dtype))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cZSBcPu597b"
      },
      "source": [
        "# Sin embargo, como nosotros somos muy ordenados y no queremos que se nos pierda\n",
        "# nada, mejor si guardamos las notas de las asignaturas (que son anuales) por \n",
        "# cuatrimestres, así será más fácil acceder a ellas si hiciese falta en un \n",
        "# futuro, ¿no os parece? ¿Cómo se os ocurre que podríamos organizarlas?\n",
        "\n",
        "# ¿Y si añadimos una dimensión a nuestro tensor 2D que indique el cuatrimestre?\n",
        "\n",
        "# Tensor 3D (matriz 3D o cubo)\n",
        "## Code ##\n",
        "## Code ##\n",
        "\n",
        "print(\"Las notas de Tristina por cuatrimestre son:\\n{}\".format(tensor_3D))\n",
        "print(\"Primer cuatrimestre:\\n{}\".format(tensor_3D[0]))\n",
        "print(\"Segundo cuatrimestre:\\n{}\".format(tensor_3D[1]))\n",
        "print(\"Dimensiones del tensor: {}\".format(tensor_3D.ndim))\n",
        "print(\"Tamaño  del tensor: {}\".format(tensor_3D.shape))\n",
        "print(\"Tipo  del tensor: {}\".format(tensor_3D.dtype))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRztNF9K-Qx_"
      },
      "source": [
        "# ¿Qué os parece? Ya tenemos perfectamente guardadas las notas de Tristina,\n",
        "# para que no se pierdan. ¿Pero qué pasa con los demás alumnos? ¿Se os ocurre\n",
        "# cómo podríamos guardarnos sus notas también?\n",
        "# ¿Y si añadimos una dimensión a nuestro tensor, de forma que podamos tener las\n",
        "# notas por cuatrimestre de cada asignatura para cada alumno?\n",
        "\n",
        "# Tensor 4D (vector de matrices 3D o vector de cubos)\n",
        "## Code ##\n",
        "## Code ##\n",
        "\n",
        "print(\"Las notas de Tristina, Facundo y Celedonio por cuatrimestre son:\\n{}\".format(tensor_4D))\n",
        "print(\"Notas de Tristina:\\n{}\".format(tensor_4D[0]))\n",
        "print(\"Notas de Facundo:\\n{}\".format(tensor_4D[1]))\n",
        "print(\"Notas de Celedonio:\\n{}\".format(tensor_4D[2]))\n",
        "print(\"Dimensiones del tensor: {}\".format(tensor_4D.ndim))\n",
        "print(\"Tamaño  del tensor: {}\".format(tensor_4D.shape))\n",
        "print(\"Tipo  del tensor: {}\".format(tensor_4D.dtype))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYU1IwoWCBaD"
      },
      "source": [
        "Y así podríamos seguir hasta el infinito añadiendo dimensiones a nuestros tensores para ser capaces de guardar más datos. Para que os hagáis una idea de cómo se suelen utilizar en el mundo del Deep Learning, los tipos más comunes de tensores son:\n",
        "\n",
        "*    **Tensores 3D**: utilizados en **series temporales**\n",
        "*    **Tensores 4D**: utilizados con **imágenes**\n",
        "*    **Tensores 5D**: utilizados con **videos**\n",
        "\n",
        "Normalmente, siempre habrá una de las dimensiones que se utilizará para almacenar las muestras de cada tipo de datos. Por ejemplo, con las imágenes:\n",
        "\n",
        "Si queremos almacenar 64 imágenes RGB de 224x224 píxels, necesitaremos un vector de matrices 3D, o lo que es lo mismo, un tensor 4D. ¿Quién sabría decirme las dimensiones que tendría nuestro vector de matrices 3D?\n",
        "\n",
        "Tenemos 64 imágenes de 224 pixels x 224 pixels x 3 canales (R, G y B). \n",
        "\n",
        "Por tanto: (64, 224, 224, 3)\n",
        "\n",
        "De acuerdo, pues ya sabemos qué es un tensor y para que sirve: ¡para mantener las notas de vuestros exámenes bien guardadas! ;D\n",
        "\n",
        "Si queréis profundizar en los tensores o más ejemplos, aquí tenéis un recurso muy bueno para ello (en inglés): [Tensors ilustrated with cats](https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo de carga de dataset externo\n",
        "\n",
        "¿Y por qué son importantes los tensores? Porque van a ser las estructuras de datos que utilizaremos para entrenar nuestras redes.\n",
        "\n",
        "Vamos a ver un ejemplo de cómo podríamos cargar un dataset de imágenes junto con sus etiquetas.\n",
        "\n",
        "Para este ejemplo vamos a utilizar imágenes provenientes de escáneres de pulmón para tratar de predecir el Covid-19. Los datos son públicos y están disponibles en el siguiente enlace:\n",
        "\n",
        "https://github.com/UCSD-AI4H/COVID-CT\n",
        "\n",
        "Este dataset consta de imágenes de pacientes patológicos y sanos. Para hacer el ejemplo más sencillo, vamos a tratar con las imágenes correspondientes a Covid19 únicamente."
      ],
      "metadata": {
        "id": "lagZCHBoujYV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv-yXA4_QuXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c86a63d-e517-43fc-b21c-0c6ed8d43acb"
      },
      "source": [
        "# Para ello, lo primero es decargarnos el dataset\n",
        "!wget -O CT_Covid.zip https://github.com/UCSD-AI4H/COVID-CT/blob/master/Images-processed/CT_COVID.zip?raw=true"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-16 16:46:34--  https://github.com/UCSD-AI4H/COVID-CT/blob/master/Images-processed/CT_COVID.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/UCSD-AI4H/COVID-CT/raw/master/Images-processed/CT_COVID.zip [following]\n",
            "--2022-05-16 16:46:34--  https://github.com/UCSD-AI4H/COVID-CT/raw/master/Images-processed/CT_COVID.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/UCSD-AI4H/COVID-CT/master/Images-processed/CT_COVID.zip [following]\n",
            "--2022-05-16 16:46:34--  https://raw.githubusercontent.com/UCSD-AI4H/COVID-CT/master/Images-processed/CT_COVID.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 47929914 (46M) [application/zip]\n",
            "Saving to: ‘CT_Covid.zip’\n",
            "\n",
            "CT_Covid.zip        100%[===================>]  45.71M   202MB/s    in 0.2s    \n",
            "\n",
            "2022-05-16 16:46:35 (202 MB/s) - ‘CT_Covid.zip’ saved [47929914/47929914]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoBobtNrRPAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f71f39-3066-45d2-984c-81d5e959d1f1"
      },
      "source": [
        "# comprobamos que tenemos el dataset descargado\n",
        "!ls -lah"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 46M\n",
            "drwxr-xr-x 1 root root 4.0K May 16 16:46 .\n",
            "drwxr-xr-x 1 root root 4.0K May 16 16:38 ..\n",
            "drwxr-xr-x 4 root root 4.0K May  3 13:41 .config\n",
            "-rw-r--r-- 1 root root  46M May 16 16:46 CT_Covid.zip\n",
            "drwxr-xr-x 1 root root 4.0K May  3 13:42 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqgap56DRoH7"
      },
      "source": [
        "# lo descomprimimos\n",
        "!unzip CT_Covid.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LBB4v_wRuzn"
      },
      "source": [
        "# ahora que ya lo tenemos descomprimido, tenemos que listar la carpeta donde se \n",
        "# encuentran todas las imágenes para después poder cargarlas. Existen varias\n",
        "# formas de hacer esto, y algunas son más eficientes que otras. Por ejemplo:\n",
        "\n",
        "# images = []\n",
        "# for img in list_imgs:\n",
        "#     images.append(load_image(img))\n",
        "\n",
        "# Esté método es el más lento que podemos utilizar, ya que cada vez que añadamos \n",
        "# una nueva imagen, Python creará una nueva lista y copiará el contenido de la\n",
        "# anterior, así que no es el método más adecuado.\n",
        "\n",
        "# Por el contrario, este es mucho mejor, ya que no se crean nuevas listas,\n",
        "# simplemente se accede a posiciones determinadas y se insertan los datos:\n",
        "\n",
        "# n_images = len(list_imgs)\n",
        "# images = np.zeros((n_images, height, width, channels), dtype=np.uint8)\n",
        "# for i, img in enumerate(list_imgs):\n",
        "#     images[i] = load_image(img)\n",
        "\n",
        "# Sin embargo, éste segundo método tiene un inconveniente: neceistamos que\n",
        "# todas las imágenes tengan el mismo tamaño, algo que no siempre pasa en\n",
        "# los datasets reales: tendremos que redimensionarlas.\n",
        "\n",
        "# Vamos a cargarlas siguiendo los dos métodos para que veais las diferencias.\n",
        "# Para ver el tiempo que tarda cada una utilizaremos la librería TQDM, que \n",
        "# crea una barra de progreso y nos informa de los tiempos."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSgRIAP_SvI7"
      },
      "source": [
        "# listamos el directorio\n",
        "import glob\n",
        "list_imgs = glob.glob('./CT_COVID/*.png')\n",
        "n_images = len(list_imgs)\n",
        "print(n_images, 'images were loaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cxJbqEtTwrT"
      },
      "source": [
        "# Perfecto, tenemos las 349 CTs que hay disponibles en el dataset. \n",
        "# Ahora, vamos a cargarlas, pero primero necesitamos saber las dimensiones\n",
        "# de las imágenes. Para ello, vamos a utilizar OpenCV.\n",
        "\n",
        "import cv2\n",
        "\n",
        "for img_path in list_imgs:\n",
        "    img = ##code##\n",
        "    print(img.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDsQRwsMVFko"
      },
      "source": [
        "# Fijaos que las imágenes tienen tamaños diferentes. Ahora, tenemos dos opciones:\n",
        "#\n",
        "# - Redimensionar las imágenes al mismo tamaño, sin mantener el ASPECT RATIO.\n",
        "#   Es el método más sencillo, pero distosionará las imágenes, por lo que puede\n",
        "#   hacer que nuestro modelo no sea todo lo bueno que podría ser.\n",
        "#\n",
        "# - Redimensionar las imágenes al mismo tamaño, manteniendo el ASPECT RATIO.\n",
        "#   Para ello, podemos redimensionar las imágenes al tamaño deseado y luego \n",
        "#   rellenar \"los huecos\" con algún valor constante, normalmente, 0.\n",
        "#\n",
        "# En este caso, por simplicidad, vamos a hacer la primera opción.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa2g67B_TI9o"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# creamos la estructura de datos donde almacenaremos las imágenes cargadas\n",
        "images = []\n",
        "\n",
        "# implementamos el bucle que recorrerá la lista e irá cargando las imágenes,\n",
        "# redimensionándolas y añadiéndolas a nuestra estructura de datos\n",
        "for i, img_path in enumerate(tqdm(list_imgs)):\n",
        "    img = ##code##\n",
        "    # opencv carga por defecto las imágenes en modo BGR\n",
        "    # esto lo tenemos que tener en cuenta, ya que si entrenamos\n",
        "    # el modelo con imágenes BGR, luego tendremos que alimentarlo\n",
        "    # con imágenes del mismo tipo para que el modelo funcione \n",
        "    # correctamente.\n",
        "    img = ##code##\n",
        "    # la redimensionamos a 96x96 (este tamaño es arbitrario porque es un ejemplo,\n",
        "    # en un caso real, estudiaríamos el tamaño que menos distorsiona la mayoría\n",
        "    # de las imágenes)\n",
        "    img = ##code##\n",
        "    # aquí incluiríamos los pasos de preprocesamiento que quisiésemos, por ejemplo:\n",
        "    # img = img / 255.  # normalizamos la imagen entre 0 y 1\n",
        "    # img = mi_funcion_random_crop(img)  # cortamos un trozo aleatoriamente\n",
        "    # añadimos la imagen a nuestra estructura de datos\n",
        "    ##code##\n",
        "\n",
        "print('Loading completed!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdblTec648Wu"
      },
      "source": [
        "type(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffga2ExT5FGM"
      },
      "source": [
        "type(images[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz8o67Jm5C4Q"
      },
      "source": [
        "images[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq56FJ_45AYN"
      },
      "source": [
        "# En el caso de cargar las imágenes así, tenemos que convertirlas a un TENSOR 4D\n",
        "# de N_Imágenes x Alto x Ancho x Canales. Ahora mismo es una lista de TENSORES 3D,\n",
        "# para convertirlo a Tensor 4D simplemente tenemos que hacer uso de la función\n",
        "# np.stack:\n",
        "\n",
        "images = np.stack(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2VZAh0P5I1m"
      },
      "source": [
        "type(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QDIU3FP5KSq"
      },
      "source": [
        "images.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-RcPEpFZKw9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# creamos la estructura de datos donde almacenaremos las imágenes cargadas\n",
        "images = ##code##\n",
        "\n",
        "# implementamos el bucle que recorrerá la lista e irá cargando las imágenes,\n",
        "# redimensionándolas y añadiéndolas a nuestra estructura de datos\n",
        "for i, img_path in enumerate(tqdm(list_imgs)):\n",
        "    img = ##code##\n",
        "    # opencv carga por defecto las imágenes en modo BGR\n",
        "    # esto lo tenemos que tener en cuenta, ya que si entrenamos\n",
        "    # el modelo con imágenes BGR, luego tendremos que alimentarlo\n",
        "    # con imágenes del mismo tipo para que el modelo funcione \n",
        "    # correctamente.\n",
        "    img = ##code##\n",
        "    # la redimensionamos a 96x96 (este tamaño es arbitrario porque es un ejemplo,\n",
        "    # en un caso real, estudiaríamos el tamaño que menos distorsiona la mayoría\n",
        "    # de las imágenes)\n",
        "    img = ##code##\n",
        "    # añadimos la imagen a nuestra estructura de datios\n",
        "    ##code##\n",
        "\n",
        "print('Loading completed!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoMkZSKXd31g"
      },
      "source": [
        "# En este caso no se nota la diferencia porque son pocas imágenes, pero cuando\n",
        "# trabajéis con grandes cantidades de datos podréis ver la diferencia.\n",
        "\n",
        "# Ahora que ya los tenemos cargados, vamos a estudiar un poco la estructura de datos\n",
        "images.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh2oOhOJ2DH5"
      },
      "source": [
        "images.min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo15GJPo2FwR"
      },
      "source": [
        "images.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9tVMzhM2MQI"
      },
      "source": [
        "images.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFrUyGqO2JWG"
      },
      "source": [
        "# Si os fijáis, nuestros datos son de tipo float64, y van de 0 a 255. \n",
        "# Normalmente, cuando trabajamos con imágenes, solemos trabajar con datos de \n",
        "# tipo uint8 que van de 0 a 255, o float que van de 0 a 1.\n",
        "# Vamos a convertir nuestra imagen a de 0 a 1.\n",
        "images = ##code##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-_s9qah4U1z"
      },
      "source": [
        "images.min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyicQVLh4Wi0"
      },
      "source": [
        "images.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Dpet446CZK"
      },
      "source": [
        "Pues así de fácil es como podemos preparar los datos para alimentar nuestra red neuronal y entrenarla :)\n",
        "\n",
        "Más adelante veremos cómo definir nuestra red neuronal y cómo entrenarla."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2-EEjlWDYgt"
      },
      "source": [
        "##2.2 Sesiones iterativas y grafos\n",
        "\n",
        "Antes os he comentado que en TensorFlow, primero se definen las operaciones a realizar y luego se ejecutan. Para ello, se usa un grafo.\n",
        "\n",
        "Un ejemplo sencillo de una suma de a + b.\n",
        "\n",
        "<img src=\"https://image.ibb.co/nkwp3y/tf_graph_2.png\" alt=\"tf_graph_2\" border=\"0\" height=\"200\">\n",
        "\n",
        "Y aquí tenéis un ejemplo un poco más complicado, para los valientes:\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/tensors_flowing.gif)\n",
        "\n",
        "Es un ejemplo de un grafo que representa la clasificación de una imagen de un número en su correspondiente clase. No es necesario que entendáis lo que está ocurriendo, simplemente que tensorflow funciona así: primero tu defines las operaciones que quieres que se realicen, junto a las variables necearias (creas el grafo) y luego lo ejecutas (con una sesión). Pero basta de cháchara, ¡vamos a ponernos manos a la obra!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HNcdMWSRH5z"
      },
      "source": [
        "\n",
        "\n",
        "Si a alguien le interesa el tema del Covid19, este enlace es un buen recurso para obtener más datos y conocer el estado del arte: https://aimi.stanford.edu/resources/covid19."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-ADlZ8eYG7h"
      },
      "source": [
        "# Lo primero que necesitamos hacer es asegurarnos de que vamos a ejecutar la \n",
        "# versión 1.x de TensorFlow. Hace aproximadamente unos tres años salió la versión\n",
        "# 2.0, que establece por defecto el \"EagerMode\" para hacer más sencilla la \n",
        "# implementación y depuración de código. \n",
        "\n",
        "# No obstante, al igual que pasó en su día con Python 2 y Python 3, con tensorflow\n",
        "# ha pasado algo similar: la versión 1.x se sigue utilizando mucho. Algunas de \n",
        "# las razones son la retro-compatibilidad y la rapidez de ejecución. En este \n",
        "# curso veremos ejemplos de ambos casos.\n",
        "\n",
        "# Vamos a ver qué versión es la que tenemos:\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYRJROCKZRWl"
      },
      "source": [
        "# Desde el 27 de marzo de 2020, Google Colab tiene activada por defecto la versión\n",
        "# 2.x. Para poder usar la 1.x debemos utilizar el siguiente comando mágico: \n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAXLHP3bZm4U"
      },
      "source": [
        "# Vamos a comprobar qué versión tenemos ahora (no cambiamos la version solo permitimos que el código de TF1 tb se pueda ejecutar):\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl8M2wYaGmu1"
      },
      "source": [
        "# Lo primero que debemos hacer es importar el paquete de Tensorflow\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf \n",
        "tf.disable_v2_behavior()\n",
        "import matplotlib.pyplot as plt # importamos también pyplot para gráficas\n",
        "%matplotlib inline\n",
        "\n",
        "# Es muy importante que conozcamos 3 conceptos básicos de TF:\n",
        "# tf.Graph: representa un conjunto de tf.Operations\n",
        "# tf.Operation: son las operaciones indicadas por las ecuaciones que escribimos\n",
        "# tf.Tensor: los resultados de las tf.Operations\n",
        "\n",
        "# En un principio, el tf.Graph es transparente a nosotros, ya que por defecto\n",
        "# existe uno donde se van añadiendo todas las operaciones que definimos:\n",
        "# tf.get_default_graph()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SX9jnzhF9hb"
      },
      "source": [
        "# Vamos a empezar con algo muy sencillo, una simple multiplicación en TensorFlow\n",
        "\n",
        "# Primero definimos los valores que queremos utilizar\n",
        "x = tf.constant(6)  # tf.Constant porque no va a cambiar durante la ejecución\n",
        "y = tf.constant(8)\n",
        "\n",
        "# Ahora definimos la operación a realizar: la multiplicación\n",
        "result = ## Code ##\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPy3q8bZMHkp"
      },
      "source": [
        "# Como podéis ver, no nos ha devuelto el resultado. Lo que ha hecho hasta ahora\n",
        "# ha sido crear el grafo. Por poner un ejemplo, es como montar un coche. Ahora \n",
        "# lo tenemos montado, pero aún no hace aquello para lo que fue diseñado, \n",
        "# desplazarse. Para ello, deberíamos encenderlo. Pues con esto es igual,\n",
        "# tenemos que encenderlo:\n",
        "sess = ##Code##  # abrimos nuestro \"coche\" y lo encendemos\n",
        "output = ##Code##   # nos ponemos en movimiento\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uwK6qYrSuD5"
      },
      "source": [
        "# Podemos ver la definición del grafo\n",
        "print(tf.get_default_graph().as_graph_def())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5PP--6XYpI7"
      },
      "source": [
        "# Como podéis observar, el grafo se compone de 2 nodos de tipo constante y uno \n",
        "# de tipo operador (la multiplicación). Sin embargo, los nombres no son muy indicativos...\n",
        "# ¿Cómo podemos nombrarlos?\n",
        "x2 = tf.constant(5.0, name='x2')\n",
        "y2 = tf.constant(6.0, name='y2')\n",
        "\n",
        "result = tf.multiply(x2, y2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8c-UxINTHBx"
      },
      "source": [
        "# Vemos la definición del grafo\n",
        "print(tf.get_default_graph().as_graph_def())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwZ66Kd5xLVl"
      },
      "source": [
        "# Una vez hemos acabado de hacer la\n",
        "# operación, necesitamos cerrar la sesión para liberar los recursos:\n",
        "\n",
        "##Code##  # apagamos y cerramos nuestro \"coche\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FstdCcqvY1Rn"
      },
      "source": [
        "# Si ahora quisieramos ejecutar de nuevo nuestras operaciones, no podríamos.\n",
        "# Sería como intentar conducir el coche desde fuera mientras está cerrado!\n",
        "\n",
        "## Code##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCaZ6H5tZabx"
      },
      "source": [
        "# Por último, también podemos indicarle a tensorflow sobre que GPU queremos que\n",
        "# se ejecute:\n",
        "\n",
        "# Para ello, primero vamos a ver cómo podemos ver una lista de los dispositivos\n",
        "# disponibles:\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def get_available_devices():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos]\n",
        "\n",
        "print(get_available_devices())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G2SwY4Ta2In"
      },
      "source": [
        "# Seleccionamos la GPU:0 y multiplicamos [3 3] por [2 2]', lo cual debería dar\n",
        "# 3x2 + 3x2 = 12. Vamos a comprobarlo:\n",
        "\n",
        "# si utilizamos la sentencia \"with ___ as ___:\"\" como en el ejemplo siguiente,\n",
        "# conseguimos que python se encargue de liberar los recursos de la sesion \n",
        "# creada por tensorflow, con lo cual no necesitamos cerrar la sesion manualmente\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  with tf.device(\"/GPU:0\"):\n",
        "    matrix1 = tf.constant([[3., 3.]])\n",
        "    matrix2 = tf.constant([[2.],[2.]])\n",
        "    product = tf.matmul(matrix1, matrix2)\n",
        "    \n",
        "  output = sess.run(product)\n",
        "  print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0I4godgZ1EK"
      },
      "source": [
        "# Vamos a crear un Tensor 1D de 32 valores igualmente espaciados entre -3 y 3\n",
        "n_values = 64\n",
        "x = ## Code ##\n",
        "\n",
        "sess = ## Code ##\n",
        "result = ## Code ##\n",
        "\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH_cClYnaby8"
      },
      "source": [
        "# Además de con sess.run(_), existen otras formas de evaluar Tensores\n",
        "## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb0uiBo7aomi"
      },
      "source": [
        "# Nos tenemos que acordar siempre de cerrar la sesión\n",
        "## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoYGiXEUatpx"
      },
      "source": [
        "# Otra opción es utilizar una sesión interactiva, lo que nos facilita la vida\n",
        "# al no tener que estar llamando constantemente al .run() para que se ejecuten \n",
        "# los resultados.  Mediante una sesión interactiva es posible establecer una \n",
        "# sesión activa por defecto mientras esta se encuentra bajo construcción.\n",
        "sess = ## Code ##\n",
        "## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tlaj5jg6a4y3"
      },
      "source": [
        "# ¿Qué os parece? Más cómodo, ¿no?\n",
        "\n",
        "# ¡Pues esto no es nada! Desde TF 2.0, el Eager Mode es el activo por defecto,\n",
        "# permitiendonos olvidarnos del grafo y comportándose como si de numpy se \n",
        "# tratara. Más adelante veremos un ejemplo de cómo es el modo de ejecución de\n",
        "# TF 2.0 ;)\n",
        "\n",
        "# Para los que seais curiosos, el gran culpable de esto fue PyTorch, otra de las\n",
        "# grandes librerías que existen para Deep Learning. Ésta, desde el principio,\n",
        "# fue mucho más intuitiva y sencilla de utilizar, implementando su \"Autograd\",\n",
        "# que lo que hace es automatizar la diferenciación de variables.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK3rEoRzgNY7"
      },
      "source": [
        " Vamos ahora a ver una tf.Operation. Para ello, vamos a usar \"x\" para crear\n",
        " y visualizar una distribución Gaussiana. Para los que no la recordéis, la fórmula de la distribución de probabilidad de Gauss es así:\n",
        "\n",
        "<img src=\"https://image.ibb.co/hTyQbJ/normal_fdp.png\" alt=\"normal_fdp\" border=\"0\">\n",
        "\n",
        "La gráfica la vamos a poder ver enseguida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuX2Xjqs4PNz"
      },
      "source": [
        "sigma = 1.0\n",
        "mean = 0\n",
        "\n",
        "# Implementamos la fórmula de una distribución Gaussiana\n",
        "g1d = ## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7In5yGzbhD5"
      },
      "source": [
        "# Podemos comprobar que nuestra tf.Operation ha sido incluída efectivamente en \n",
        "# nuestro tf.Graph:\n",
        "\n",
        "if g1d.graph is tf.get_default_graph():\n",
        "  print('Todo bien')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZwcUXOkbw8g"
      },
      "source": [
        "# Ejecutando el tf.Graph obtenemos el resultado, y con pyplot lo visualizamos:\n",
        "## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX8QHdMOd9vJ"
      },
      "source": [
        "# ¿Qué dimensiones tiene \"g1d\"?\n",
        "## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53bEoZXVeDDi"
      },
      "source": [
        "# Habrá momentos en los que no sepamos las dimensiones de una variable hasta que\n",
        "# sea ejecutada la operación que devuelve su valor. Para estos casos, podemos \n",
        "# utilizar tf.shape(variable), que nos devuelve un Tensor que calculará en tiempo\n",
        "# de ejecución las dimensiones de nuestro resultado.\n",
        "\n",
        "# Esto se conoce como 'static shape' y 'dinamic shape', donde static se calcula\n",
        "# teniendo en cuenta las dimensiones de los Tensores y Operaciones involucrados,\n",
        "# y la dinámica en tiempo de ejecución.\n",
        "\n",
        "# ¿Qué pasa si definimos x como un \"placeholder\"? Un place holder es como una \n",
        "# reserva, indica que ahí habrá un tensor, pero no es necesario definirlo en \n",
        "# ese momento. Por ejemplo, definiendo:\n",
        "\n",
        "x = ## Code ##\n",
        "\n",
        "# sabemos que x va a albergar un tensor 1D de 5 dimensiones, como confirma \n",
        "# x.get_shape():\n",
        "\n",
        "print(x.get_shape())\n",
        "\n",
        "# pero no sabemos qué valores van a formarla hasta que no se lo indiquemos.\n",
        "\n",
        "# Diferencias entre placeholder y variable:\n",
        "\n",
        "# Variables\n",
        "\n",
        "#  - Se utilizan para alojar parámetros que se aprenden durante el entrenamiento\n",
        "#  - Por tanto, los valores se derivan del entrenamiento\n",
        "#  - Requieren que se les asigne un valor inicial (puede ser aleatorio)\n",
        "\n",
        "# Placeholders\n",
        "\n",
        "#  - Reservan espacio para datos (por ejemplo, para los pixels de una imagen)\n",
        "#  - No requieren que se les asigne un valor iniciar (aunque se puede)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HwYg186kA6k"
      },
      "source": [
        "# Sin embargo, éste es el valor estático de las dimensiones de x. ¿Qué ocurre\n",
        "# si aplicamos un tf.unique() sobre x?\n",
        "y, _ = ## Code ##\n",
        "print(y.get_shape())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGCCOqWZmQ51"
      },
      "source": [
        "# Parece que está confundido, como Dinio por la noche. Lo que pasa, es que \n",
        "# tf.unique() devuelve los valores únicos de x, que en un principio no se \n",
        "# saben, ya que x está definida como un placeholder, y un placeholder no tiene\n",
        "# por qué estar definido hasta el momento de la ejecución, como hemos dicho\n",
        "# antes. De hecho, vamos a ver qué ocurre si alimentamos \"x\" con dos valores\n",
        "# distintos:\n",
        "\n",
        "## Code ##\n",
        "## Code ##\n",
        "## Code ##\n",
        "  \n",
        "# ¡Fijaos! El tamaño de y cambia dependiendo de lo que devuelve tf.unique(). A \n",
        "# esto se le llama \"dynamic shape\", y siempre está definido, nunca os devolverá\n",
        "# un interrogante por respuesta. Gracias a esto, TensorFlow soporta operaciones\n",
        "# como tf.unique() que pueden tener resultados de tamaños variables."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAd6y44BkTGF"
      },
      "source": [
        "# Así que ya sabéis, siempre que utilicéis operaciones con salidas variables\n",
        "# tendréis que usar tf.shape(variable) para calcular el dynamic shape de un \n",
        "# tensor\n",
        "\n",
        "sy = tf.shape(y)\n",
        "# Devuelve una lista con las dimensiones\n",
        "## Code ##\n",
        "## Code ##\n",
        "\n",
        "# Accedemos a la dimension que nos interesa\n",
        "## Code ##\n",
        "## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRSp25a-eVyz"
      },
      "source": [
        "# Así, podemos realizar operaciones después teniendo en cuenta el tamaño de la \n",
        "# salida de nuestra operación, el cual desconocemos previamente\n",
        "\n",
        "# Recordad que shape devuelve una lista con las dimensiones de nuestro tensor\n",
        "## Code ##\n",
        "## Code ##\n",
        "\n",
        "# Es necesario que especifiquemos que dimensiones nos interesan\n",
        "## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg6piUw8eYkl"
      },
      "source": [
        "# Vamos ahora a ver una distribución Gaussiana en 2D\n",
        "g1d_r = ## Code ##\n",
        "print(##Code##)\n",
        "print(##Code##)\n",
        "\n",
        "# Multiplicamos el vector fila de la Gaussiana 1D por el columna para obtener la\n",
        "# versión 2D\n",
        "g2d = ##Code##\n",
        "\n",
        "# Visualizamos la Gaussiana 2D\n",
        "plt.imshow(##Code##)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j79Av-xWvLu"
      },
      "source": [
        "# Podemos ver un listado de las operaciones incluídas en nuestro tf.Graph\n",
        "ops = ## Code ##\n",
        "print([op.name for op in ops])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgUfBc9A6lsL"
      },
      "source": [
        "## ¡Enhorabuena! Ya habéis aprendido los conceptos básicos de TensorFlow.\n",
        "\n",
        "Vamos a ver ahora ejemplos del mundo real, para que veáis que esto no solo sirve para dibujar Gaussianas.\n",
        "\n",
        "## 2.3 Problemas de optimización. Descento de gradiente\n",
        "Como veremos a lo largo de las sesiones, una red neuronal no es otra cosa que un optimizador de funciones. ¿Y para qué podríamos utilizar este optimizador de funciones? Vamos a ver varios ejemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-J6OIdyESot"
      },
      "source": [
        "### 2.3.1 Ejercicio 1\n",
        "\n",
        "Netflix ha decidido colocar otro de sus famosos carteles publicitarios en un edificio.\n",
        "\n",
        "<img src=\"https://image.ibb.co/c199mJ/cartel_netflix.jpg\" alt=\"cartel_netflix\" border=\"0\" width=\"300\">\n",
        "\n",
        "Han decidido que el cartel publicitario tiene que cubrir una superficie de 600 metros cuadrados, dejando un margen de 2 metros arriba y abajo y de 4 metros a izquierda y derecha para publicidad. Sin embargo, no les han comunicado las dimensiones de la fachada del edificio. Podríamos mandar un email al propietario y preguntarle, pero como sabemos matemáticas podemos ahorrárnoslo. ¿Cómo podemos averiguar las dimensiones del edificio?\n",
        "\n",
        "<img src=\"https://image.ibb.co/iq5qRJ/opt_problem.png\" alt=\"opt_problem\" border=\"0\" width=\"400\">\n",
        "\n",
        "Veamos, la superficie completa del edificio será:\n",
        "\n",
        "**Ancho = 4 + x + 4 = x + 8**\n",
        "\n",
        "**Alto = 2 + y + 2 = y + 4**\n",
        "\n",
        "**S = Ancho x Alto = (x+8)(y+4)**\n",
        "\n",
        "Y tenemos la condición de que: **xy = 600**\n",
        "\n",
        "Esto nos permite formar un sistema de ecuaciones, en el que sustituyendo:\n",
        "\n",
        "xy = 600 -> x = 600/y\n",
        "\n",
        "S(y) = ( 600/y + 8)(y+4) = 600 + 8y + 4·600/y + 32 = 632 + 8y + 2400/y\n",
        "\n",
        "En un problema de optimización, se utiliza la información de la pendiente de la función, es decir, la derivada, para calcular su mínimo. Tenemos que igualar la primera derivada a 0 y luego comprobar que la segunda derivada es positiva. Así, en este caso:\n",
        "\n",
        "**S'(y) = 8-2400/y²**\n",
        "\n",
        "**S''(y) = 4800/y³**\n",
        "\n",
        "S'(y) = 0 -> 0 = 8-2400/y² -> 8 = 2400/y² -> y² = 2400/8 = 300 -> **y =** sqrt(300) = sqrt(100 · 3) = sqrt(100)·sqrt(3) = 10·sqrt(3) = **17.32** (descartamos el signo negativo por no tener sentido físico)\n",
        "\n",
        "Sustituyendo en x:\n",
        "\n",
        "**x =** 600 / 10·sqrt(3) = 60 / sqrt(3) = 60·sqrt(3) / sqrt(3)·sqrt(3) = 60·sqrt(3) / 3 = 20·sqrt(3) = **34.64**\n",
        "\n",
        "Como para y = 17.32 -> S''(y) = 0.9238 > 0, hemos encontrado la solución mínima.\n",
        "\n",
        "Con lo cual, las dimensiones del edificio son:\n",
        "\n",
        "**Ancho: x + 8 = 42.64 m**\n",
        "\n",
        "**Alto: y + 4 = 21.32 m**\n",
        "\n",
        "\n",
        "¿Habéis visto lo útiles que son las derivadas? Acabamos de solucionar este problema **analíticamente**. Lo hemos hecho así porque es sencillo y podemos, pero existen muchos problemas para los que es muy computacionalmente costoso solucionarlos analíticamente, por lo que se emplean métodos numéricos. Uno de estos métodos es el **Descenso del gradiente**. ¿Y qué es el descenso del gradiente?\n",
        "\n",
        "Veamoslo rápidamente con un ejemplo.\n",
        "\n",
        "El descenso del gradiente es el mecanismo que hace que una red neuronal aprenda. Al fin y el cabo, podemos ver el descenso del gradiente como un algoritmo de optimización que permite minimizar cualquier función (siempre que sea diferenciable, es decir, que podamos calcular sus derivadas.\n",
        "\n",
        "<center><img src=\"http://www.cs.us.es/~fsancho/images/2017-02/gradient_descent.gif\" border=\"0\"></center>\n",
        "\n",
        "No os preocupéis, lo veremos con más detalle en próximas sesiones!\n",
        "\n",
        "Ahora que ya conocemos una forma de encontrar el valor óptimo a nuestra funcion, ¿Qué os parece si solucionamos este problema esta vez **numéricamente con Tensorflow**? ¡Vamos allá!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIClIu8I62dV"
      },
      "source": [
        "# Tenemos el sistema de ecuaciones:\n",
        "# S = (x+8)(y+4)\n",
        "# xy = 600\n",
        "\n",
        "# Que sustituyendo, nos da la siguiente ecuación:\n",
        "# S(y) = 632 + 8y + 2400/y\n",
        "\n",
        "# Que es la que queremos minimizar\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47Md2tE2dGY_"
      },
      "source": [
        "Por supuesto, ahora quedaría calcular **x** sustituyendo en x = 600/y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-ihkupwdU9X"
      },
      "source": [
        "## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEKkR6AMdrR7"
      },
      "source": [
        "Vamos a hacer una última comprobación dibujando la función:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT5d9IQ1g4QV"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y = ## Code ##\n",
        "s = ## Code ##\n",
        "## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvBbbaqPkY13"
      },
      "source": [
        "print(\"El mínimo de la función se encuentra en {}\".format(np.min(s)))\n",
        "min_s = ## Code ##\n",
        "s_min_idx = ## Code ##\n",
        "y_min = y## Code ##\n",
        "print(\"El valor de y que consigue el mínimo es {}\".format(y_min[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to1lM2_n0M2O"
      },
      "source": [
        "¡Vamos a ver otro ejemplo!\n",
        "\n",
        "### 2.3.3 Ejercicio 2\n",
        "\n",
        "En este caso, queremos encontrar el mínimo de la función y=log²(x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDMg7mZs0Lxr"
      },
      "source": [
        "# Hallar el mínimo de la función y=log(x)^2\n",
        "## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaOkcoPX1WbO"
      },
      "source": [
        "\n",
        "\n",
        "A ver... visualizemos la función:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juQYjXU51OIt"
      },
      "source": [
        "## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mcxneho21S_"
      },
      "source": [
        "\n",
        "### 2.3.2 Ejercicio 3. Regresión Lineal\n",
        "A continuación, vamos a realizar una **regresión lineal** sobre un determinado conjunto de datos $X$. Dicha regresión lineal vendrá dada por la ecuación de la recta de la forma $\\hat{Y} = XW + b$. La solución óptima del ajuste de la recta ($W_{opt}, b_{opt}$) sobre la distribución de datos se puede obtener minimizando la función error cuadrático medio definida como  $MSE = \\frac{1}{N}\\sum_{i=1}^{N}(\\hat{y}_i - y_i)^2$ donde $N$ es el número de muestras del conjunto de datos e $Y$ es un vector con el *ground truth* de los mismos.\n",
        "\n",
        "Vamos a ello! Veamos como ajustar una recta a un conjunto de datos que representan la inteligencia de cada uno de los personajes de Los Simpsons, desde Ralph Wiggum hasta el Doctor Frink.\n",
        "\n",
        "<img src=\"https://image.ibb.co/kXFbyn/ralph.gif\" alt=\"ralph\" border=\"0\" height=200>&nbsp;\n",
        "<img src=\"https://image.ibb.co/nMK1yJ/frink.gif\" alt=\"frink\" border=\"0\" height=200>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuw2eV6965pO"
      },
      "source": [
        "# Vamos a ver cual es la distribución de las inteligencias según las edades\n",
        "# normalizadas de 0 a 1, donde Maggie es la más pequeña, y Montgomery Burns el\n",
        "# más mayorS\n",
        "n_observations = 50\n",
        "## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmMtQBKi7z0T"
      },
      "source": [
        "# Ahora necesitamos dos tf.placeholders, para la entrada y la salida de nuestro\n",
        "# algoritmo de regresión. Como ya sabéis, los placeholders son variables a las \n",
        "# que no es necesario asignarle un valor hasta que se ejecute el grafo\n",
        "X = ## Code ##\n",
        "Y = ## Code ##\n",
        "\n",
        "# Vamos a intentar optimizar una recta de regresión lineal:\n",
        "# min_(W,b) ||(X*w + b) - y||^2\n",
        "# Necesitamos 2 variables, W (los pesos) y b (la bias). Los elementos de tipo\n",
        "# tf.Variable necesitan una inicialización, y su tipo no se puede cambiar\n",
        "# después. Lo que si podemos cambiar es su valor, mediante el método \"assign\"\n",
        "\n",
        "W = ## Code ##\n",
        "b = ## Code ##\n",
        "Y_pred = ## Code ##\n",
        "\n",
        "# Definimos la función de coste o pérdidas como la diferencia entre nuestras\n",
        "# predicciones y los valores reales\n",
        "loss = ## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZsQMjn694Yz"
      },
      "source": [
        "# Definimos ahora el método de optimización, que va a ser el Descenso del \n",
        "# gradiente. Más adelante entenderemos como funciona. Básicamente, calcula\n",
        "# la variación de cada peso con respecto al error total, y actualiza cada peso \n",
        "# de forma que disminuya el error total en las subsiguientes iteraciones.\n",
        "# El learning rate indica cómo de brusca es la actualización de los pesos\n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = ## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQML8lzU3O6L"
      },
      "source": [
        "# Definimos el número de iteraciones permitidas y comenzamos la inicialización\n",
        "# usando la GPU\n",
        "n_epochs = 1000\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  with tf.device(\"/GPU:0\"):\n",
        "\n",
        "    # Inicializamos todas las variables definidas\n",
        "    ## Code ##\n",
        "\n",
        "    # Empieza el ajuste\n",
        "    ## Code ##\n",
        "\n",
        "      # Imprimimos las pérdidas cada 20 épocas\n",
        "      ## Code ##\n",
        "\n",
        "      # Condición de terminación\n",
        "      ## Code ##\n",
        "\n",
        "    # Dibujamos el resultado\n",
        "    ## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ar-ifeED5P7"
      },
      "source": [
        "# ¿Y si quisiéramos ejecutarlo en la CPU?\n",
        "n_epochs = 1000\n",
        "with tf.Session() as sess:\n",
        "  with tf.device(##Code##):\n",
        "\n",
        "\n",
        "    # Inicializamos todas las variables definidas\n",
        "    ## Code ##\n",
        "\n",
        "    # Empieza el ajuste\n",
        "    ## Code ##\n",
        "\n",
        "      # Imprimimos las pérdidas cada 20 épocas\n",
        "      ## Code ##\n",
        "\n",
        "      # Condición de terminación\n",
        "      ## Code ##\n",
        "\n",
        "    # Dibujamos el resultado\n",
        "    ## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rjtlGVXGyoi"
      },
      "source": [
        "\n",
        "\n",
        "### 2.3.4 Ejercicio 4. Regresión logística\n",
        "Veamos ahora cómo clasificar imágenes de dígitos con una regresión logística.\n",
        "\n",
        "Vamos a utilizar el archiconocido dataset MNIST, que es como el \"Hola mundo\" de los datasets:\n",
        "\n",
        "<img src=\"https://image.ibb.co/gkZAD8/mnist.jpg\" alt=\"mnist\" border=\"0\" height=\"300\">\n",
        "<img src=\"https://image.ibb.co/kgLyOT/mnist_image.png\" alt=\"mnist_image\" border=\"0\" height=\"300\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tknM5oPIS2nt"
      },
      "source": [
        "# Importamos el dataset que vamos a utilizar: el MNIST\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Cargamos el dataset \n",
        "(x_train, y_train), (x_test, y_test) = ## Code ##\n",
        "x_train, x_val, y_train, y_val = ## Code ##\n",
        "\n",
        "# Normalizamos el dataset\n",
        "x_train = ## Code ##\n",
        "x_test = ## Code ##\n",
        "x_val= ## Code ##\n",
        "\n",
        "# El dataset ya está dividido en train, validation y test. Dentro de cada uno\n",
        "# de estos subsets ver el número de ejemplos y las dimensiones: \n",
        "print(\"El conjunto de entrenamiento tiene dimensiones: \", x_train.shape)\n",
        "print(\"El conjunto de validación tiene dimensiones: \",x_val.shape)\n",
        "print(\"El conjunto de test tiene dimensiones: \",x_test.shape)\n",
        "\n",
        "#Hacemos lo mismo para las etiquetas.\n",
        "print(\"El conjunto de entrenamiento (etiquetas) tiene dimensiones: \", y_train.shape)\n",
        "print(\"El conjunto de validación (etiquetas) tiene dimensiones: \",y_val.shape)\n",
        "print(\"El conjunto de test (etiquetas) tiene dimensiones: \",y_test.shape)\n",
        "\n",
        "# Cada etiqueta debería ser guardada en un vector de longitud = N_CLASES, con todo 0s excepto para \n",
        "# el índice que indica la clase a la que pertenece la imágen, que contiene un 1)\n",
        "# Por ejemplo, si tenemos 10 clases (números del 0 al 9), y la etiqueta \n",
        "# pertenece al número 5:\n",
        "# label = [0 0 0 0 0 1 0 0 0 0]\n",
        "#Esto se llama one-hot encodding, cambiamos el formato de la etiquetas\n",
        "y_train = ## Code ##\n",
        "y_val = ## Code ##\n",
        "y_test = ## Code ##\n",
        "\n",
        "print(\"El conjunto de entrenamiento (etiquetas) en one-hot encoding tiene dimensiones: \", y_train.shape)\n",
        "print(\"El conjunto de validación (etiquetas) en one-hot encoding tiene dimensiones: \",y_val.shape)\n",
        "print(\"El conjunto de test (etiquetas) en one-hot encoding tiene dimensiones: \",y_test.shape)\n",
        "\n",
        "# Veamos algunas de las imágenes del dataset...\n",
        "# Para ello, solo necesitamos acceder a un vector de nuestra matrix y \n",
        "# redimensionarlo a 28x28\n",
        "plt.subplot(131)\n",
        "plt.imshow(np.reshape(x_train[0, :], (28, 28)), cmap='gray')\n",
        "plt.subplot(132)\n",
        "plt.imshow(np.reshape(x_train[27500, :], (28, 28)), cmap='gray')\n",
        "plt.subplot(133)\n",
        "plt.imshow(np.reshape(x_train[41000, :], (28, 28)), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parámetros\n",
        "learning_rate = 0.01\n",
        "n_epochs = 10\n",
        "batch_size = 100\n",
        "# Reducimos tamaño de entrenamiento para que vaya más rápido\n",
        "size_train=1000\n",
        "x_train=x_train[0:size_train]\n",
        "y_train= y_train[0:size_train]"
      ],
      "metadata": {
        "id": "172Dm15kjz6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L7KFmKSVtKZ"
      },
      "source": [
        "# Ya hemos visto un poco en qué consiste el dataset del MNIST. \n",
        "# Convertimos las imágenes a vectores, dado que aún no hemos visto cómo podemos implementar un modelo que trabaje con imágenes\n",
        "x_train = tf.reshape(x_train, shape=(size_train, -1)) # Nuestros datos ya están en formato [N_instancias, variables] (nº instancias, 784 (28+28) pixels).\n",
        "x_val = tf.reshape(x_val, shape=(9000, -1)) # Nuestros datos ya están en formato [N_instancias, variables] (nº instancias, 784 (28+28) pixels).\n",
        "x_test = tf.reshape(x_test, shape=(10000, -1)) # Nuestros datos ya están en formato [N_instancias, variables] (nº instancias, 784 (28+28) pixels).\n",
        "\n",
        "# Vamos a crear nuestro regresor:\n",
        "\n",
        "# Cuando ejecutamos en grafo, lo primero es que creemos el placeholder \n",
        "# para nuestros datos de entrada y salida. En este caso, la entrada va a ser \n",
        "# un conjunto de vectores de tamaño 768 (vamos a pasarle varias imágenes \n",
        "# a la vez a nuestro regresor, de esta forma, cuando calcule el gradiente \n",
        "# se basará en varias imágenes, con lo que la  estimación será más precisa\n",
        "# que si utilizase solo una). La salida tendrá dimensión 10, que son las 10 \n",
        "# clases en las que podemos clasificar\n",
        "\n",
        "n_input = 784  # Número de características de los datos: nº píxels de la imagen\n",
        "n_output = 10  # Número de clases: del 0 al 9\n",
        "\n",
        " # Creamos el placeholder de entrada a nuestro algoritmo\n",
        "net_input = ## Code ##\n",
        "# Necesitamos también un placeholder para la etiqueta de la imagen, con la que \n",
        "# compararemos nuestra predicción\n",
        "y_true = ## Code ##\n",
        "\n",
        "# Ahora tenemos que definir la ecuación de nuestra regresión. En este caso vamos\n",
        "# a definir nuestra regresión como y = W*x + b\n",
        "W = ## Code ##\n",
        "b = ## Code ##\n",
        "\n",
        "# Como la salida es multiclase, necesitamos una función que nos devuelva las \n",
        "# probabilidades de una imagen de pertenecer a cada de las posibles clases. Lo\n",
        "# ideal, además, es que estas probabilidades sumasen 1.\n",
        "# Por ejemplo, si metemos una imagen con un 5, una posible salida sería:\n",
        "# [0.05 0.05 0.05 0.05 0.05 0.55 0.05 0.05 0.05 0.05]\n",
        "# cuya suma de probabilidades es 1, y la clase con la mayor probabilidad es 5.\n",
        "# Aplicamos la función softmax para normalizar las probabilidades de salida\n",
        "net_output = ## Code ##\n",
        "print(\"Tamaño del input es: {}\".format(net_input)) # Dimensiones  axb\n",
        "print(\"Tamaño los pesos es: {}\".format(net_output)) # El prodcuto tendrá dimensiones  axc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-dt_6lvXUP8"
      },
      "source": [
        "### La función softmax\n",
        "<img src=\"https://image.ibb.co/ethRg7/softmax.png\" alt=\"softmax\" border=\"0\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnYdIg6CXcrQ"
      },
      "source": [
        "# Ahora, definimos nuestra función de pérdidas: esta vez, la cros-entropía\n",
        "# a veces la llaman loss, a veces cost => es lo mismo\n",
        "cost = ## Code ##\n",
        "\n",
        "# Comprobamos si nuestra predicción es igual a la etiqueta\n",
        "idx_prediction = ## Code ##\n",
        "idx_label = ## Code ##\n",
        "correct_prediction = ## Code ##\n",
        "\n",
        "# Definimos nuestra medida de precisión como el número de aciertos con respecto \n",
        "# al número de muestras predichas\n",
        "accuracy = ## Code ##\n",
        "\n",
        "# Ahora indicamos que queremos minimizar nuestra función de pérdidas (la entropía\n",
        "# cruzada) usando el algoritmo del descenso del gradiente y con una tasa de \n",
        "# aprendizaje = 0.01.\n",
        "optimizer =## Code ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pues ya lo tenemos todo listo! Solo nos queda ejecutar el grafo:\n",
        "# Esto es necesario para poder escribir continuamente en la misma línea\n",
        "from IPython.display import clear_output\n",
        "\n",
        "#Definimos el número de batches totales\n",
        "total_batch = size_train / batch_size\n",
        "\n",
        "# inicializamos las variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "  sess.run(init)\n",
        "\n",
        "  # Ahora entrenamos nuestro regresor\n",
        "  #ind=0\n",
        "  for sample_i in range(0,x_train.shape[0],batch_size):\n",
        "    sample_x = x_train[sample_i:sample_i+batch_size]\n",
        "    sample_y = y_train[sample_i:sample_i+batch_size]\n",
        "    #Ejecutamos la sesión\n",
        "    ## code ##\n",
        "    # Comprobamos cómo va funcionando nuestro regresor con el conjunto de validación\n",
        "    if  sample_i  % batch_size == 0:\n",
        "      val_acc = ## code ##\n",
        "      print(\"({}/{}) Acc: {}\".format(sample_i + batch_size, size_train, val_acc))\n",
        "\n",
        "  # Cuando ya ha visto todas las muestras del conjunto de entrenamiento, \n",
        "  # mostramos la precisión final con el conjunto de test\n",
        "  print('Test accuracy: ', ## code ##)"
      ],
      "metadata": {
        "id": "9hsCU1sTlGn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWl8_jUglKeZ"
      },
      "source": [
        "Hemos implementado una regresión logística, con esta formula: y = G(Wx + b), donde G = softmax() en vez de la típica G = sigmoid(). Pues bien, si os fijáis en la siguiente imagen, en la que se define **el perceptrón** (una red neuronal de una sola capa) podréis comprobar como output = Activation_function(Wx). ¿Lo véis? ¡Solo falta la bias! ¿Y os fijáis que en la entrada hay un 1? Con lo cual, el peso w0 no se ve multiplicado por nada. ¡Exacto! El peso w0 es la bias, que aparece con esta notación simplemente para poder implementarlo en forma de multiplicación matricial.\n",
        "\n",
        "<img src=\"https://image.ibb.co/kzdu17/perceptron_schematic.png\" alt=\"perceptron_schematic\" border=\"0\">\n",
        "\n",
        "Con lo cual, lo que acabamos de implementar es un perceptron, con un batch_size = 100, 1 época, gradien descent como optimizador y softmax como función de activación.\n",
        "\n",
        "**En la siguiente sesión veremos las redes neuronales en profundidad, ¡por hoy ya habéis tenido bastante!**"
      ]
    }
  ]
}