{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is_eITHCjKjJ"
      },
      "source": [
        "# Módulo 7. Modelos secuenciales \n",
        "\n",
        "¡Bienvenidos al séptimo módulo! Ya sabemos lo que son las redes neuronales, las redes convolucionales y cómo optimizar sus parámetros al máximo usando diferentes métodos, desde fuerza bruta hasta algoritmos genéticos. Así que en este módulo vamos a trabajar con arquitecturas de red totalmente diferentes, los **modelos secuenciales**.\n",
        "\n",
        "En este módulo estudiaremos lo siguiente:\n",
        "\n",
        "1. **Introducción**\n",
        "\n",
        "2. **Redes recurrentes**\n",
        "\n",
        "3. **Long Short-Term Memory (LSTM) networks**\n",
        "\n",
        "4. **Ejemplos de aplicación**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK8MAsqOtomN"
      },
      "source": [
        "## **1. Introducción**\n",
        "\n",
        "Los modelos de secuencias (en inglés sequence models) son las técnicas utilizadas cuando el orden y la secuencia de los datos aportan mucho valor predictivo. \n",
        "\n",
        "**¿A qué nos referimos con secuencia?**\n",
        "\n",
        "Una secuencia es una serie de elementos que se suceden unos a otros y guardan relación entre sí.\n",
        "\n",
        "Voy a transcribirlo a un ejemplo fácil de asimilar. Cuando alguien nos pregunta el abecedario sabemos responder rápidamente sin dudarlo. Sin embargo, cuando tenemos que empezar el abecedario por una letra aleatoria nos cuesta un poco más, y si tenemos que decirlo de atrás para delante ya ni te cuento. Esto se debe a que tenemos metido en la cabeza el abecedario como una secuencia de letras y lo tenemos memorizado así. Esto es porque tanto las letras que preceden como las posteriores nos aportan mucha información para saber cuál toca.\n",
        "\n",
        "\n",
        "\n",
        "Y, **¿Por qué no podemos utilizar modelos tradicionales o redes neuronales normales?**\n",
        "\n",
        "Las redes neuronales estándar (NN a partir de ahora) presentan una serie de debilidades a la hora de afrontar este tipo de problemas:\n",
        "\n",
        " \n",
        "*   Las NN están diseñadas para tener siempre el mismo tamaño de input y de output siempre. Sin embargo, en los problemas secuenciales, los inputs y los outputs pueden tener tamaños muy diferentes dependiendo de la observación. Por ejemplo, a la hora de traducir, cuando queremos traducir tres palabras tenemos un input y output mucho más pequeños que cuando queremos traducir un párrafo.\n",
        "*  Las NN no comparten características entre las diferentes posiciones. Es decir, las NN asumen que cada input (y output) es independiente uno del otro, es decir, **no tienen ningún tipo de memoria**.\n",
        "\n",
        "\n",
        "Vamos a ver como actúan las redes neuronales y su diferencia con las secuenciales siguiendo un ejemplo: **el del perfecto compañero de piso**. \n",
        "\n",
        "Imagináos que vivimos en un piso y que nuestro compañero de piso es estupendo, porque cada día cocina una cosa distinta según el tiempo que hace, soleado o lluvioso.\n",
        "\n",
        "<img src=\"https://image.ibb.co/gSmT3J/nnintro_perfect_roomate.png\" alt=\"nnintro_perfect_roomate\" border=\"0\" height=\"200\">\n",
        "\n",
        "<img src=\"https://image.ibb.co/nCSCVy/nnintro_perfect_roomate_weather.png\" alt=\"nnintro_perfect_roomate_weather\" border=\"0\" height=\"200\">\n",
        "\n",
        "Si hace sol, tarta de manzana, si llueve, hamburguesa. La forma de codificar las comidas y el tiempo es la siguiente:\n",
        "\n",
        "<img src=\"https://image.ibb.co/h32Pcd/nnintro_perfect_roomate_vectors.png\" alt=\"nnintro_perfect_roomate_vectors\" border=\"0\" height=\"200\">\n",
        "\n",
        "Y esto es lo que hace nuestra red neuronal:\n",
        "\n",
        "<img src=\"https://image.ibb.co/exmZtJ/nnintro_example_gif.gif\" alt=\"nnintro_example_gif\" border=\"0\" height=\"200\">\n",
        "\n",
        "Acordaos que lo que aprenden las redes neuronales son unos pesos que pueden expresarse como una matriz. Aquí tenemos la nuestra:\n",
        "\n",
        "* Si el día es soleado\n",
        "\n",
        "<img src=\"https://image.ibb.co/d98fHd/nnintro_perfect_roomate_applepie.png\" alt=\"nnintro_perfect_roomate_applepie\" border=\"0\" height=\"150\">\n",
        "\n",
        "* Si el día está nublado\n",
        "\n",
        "<img src=\"https://image.ibb.co/n2JQiJ/nnintro_perfect_roomate_burguer.png\" alt=\"nnintro_perfect_roomate_burguer\" border=\"0\" height=\"150\">\n",
        "\n",
        "Y nuestra matriz de pesos vista en forma de grafo:\n",
        "\n",
        "<img src=\"https://image.ibb.co/m9LD3J/nnintro_perfect_roomate_matrix_graph.png\" alt=\"nnintro_perfect_roomate_matrix_graph\" border=\"0\" height=\"200\">\n",
        "\n",
        "\n",
        "Vale, hasta aquí nada nuevo, ¿verdad? Pues vamos a ver qué es lo que añaden las arquitectura más común basada en modelos secuenciales, las **redes recurrentes**, en inglés Recurrent neural networks (RNN).\n",
        "\n",
        "## **2. Redes recurrentes**\n",
        "\n",
        "Pongamos que ahora nuestro querido compañero de piso no solamente basa la decisión de qué cocina en el tiempo, si no que ahora simplemente se fija en lo que cocinó ayer.\n",
        "\n",
        "<img src=\"https://image.ibb.co/fVD3nd/rnnintro_perfect_roomate_food_cycle.png\" alt=\"rnnintro_perfect_roomate_food_cycle\" border=\"0\" height=\"75\">\n",
        "\n",
        "Pues la red encargada de conseguir predecir lo que cocinará vuestro querido *roommate* mañana en función de lo que cocinó hoy es una:\n",
        "\n",
        "<img src=\"https://image.ibb.co/c0GpSd/rnnintro_perfect_roomate_food_cycle_gif.gif\" alt=\"rnnintro_perfect_roomate_food_cycle_gif\" border=\"0\" height=\"200\">\n",
        "\n",
        "Que se puede expresar como la matriz que podéis ver a continuación, y funciona así:\n",
        "\n",
        "<img src=\"https://image.ibb.co/k0jOLy/nnintro_rnn_gif.gif\" alt=\"nnintro_rnn_gif\" border=\"0\" height=\"250\">\n",
        "\n",
        "Que en forma de grafo, se puede expresar así:\n",
        "\n",
        "<img src=\"https://image.ibb.co/fsTHfy/nnintro_rnn_graph_gif.gif\" alt=\"nnintro_rnn_graph_gif\" border=\"0\" height=\"250\">\n",
        "\n",
        "Así que realmente, lo que tenemos al final es esto:\n",
        "\n",
        "<img src=\"https://image.ibb.co/iL2ztJ/nnintro_rnn_graph.png\" alt=\"nnintro_rnn_graph\" border=\"0\" height=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFsPRqJ8mJS3"
      },
      "source": [
        "Una vez que se entiende en funcionamiento básico de una red recurrente, ¡vamos a complicarlo un poco más! Imagináos ahora que vuestro compañero decide lo que cocina en función de lo que cocinó ayer y del tiempo que hace. En concreto, si sale el día soleado, se pasa el día en la terracita con una buena birra en la mano, con lo cual no cocina, así que comemos lo mismo de ayer. Pero si sale lluvioso, se queda en casa y si que cocina. Sería algo así:\n",
        "\n",
        "<img src=\"https://image.ibb.co/bUJ3nd/rnn_food_weather.png\" alt=\"rnn_food_weather\" border=\"0\" height=\"150\">\n",
        "\n",
        "Con lo que tenemos una parte que nos modela lo que nos tocaría comer en función de lo que comimos ayer:\n",
        "\n",
        "<img src=\"https://image.ibb.co/gL77DJ/nnintro_rnn_food_weather_gif.gif\" alt=\"nnintro_rnn_food_weather_gif\" border=\"0\" height=\"250\">\n",
        "\n",
        "Y otra que nos dice si nos cocinan o si se va al bar:\n",
        "\n",
        "<img src=\"https://image.ibb.co/jC6g7d/rnn_food_weather_gif.gif\" alt=\"rnn_food_weather_gif\" border=\"0\" height=\"250\">\n",
        "\n",
        "Por lo que al final, tenemos una combinación de las dos:\n",
        "\n",
        "<img src=\"https://image.ibb.co/cdcA0y/rnn_food_weather_matrices.png\" alt=\"rnn_food_weather_matrices\" border=\"0\" height=\"250\">\n",
        "\n",
        "Y las operaciones Add y Merge son las siguientes:\n",
        "\n",
        "<img src=\"https://image.ibb.co/ciutnd/rnn_food_weather_add.png\" alt=\"rnn_food_weather_add\" border=\"0\" height=\"200\">\n",
        "\n",
        "<img src=\"https://image.ibb.co/fnKhfy/rnn_food_weather_merge.png\" alt=\"rnn_food_weather_merge\" border=\"0\" height=\"200\">\n",
        "\n",
        "\n",
        "Y aquí podéis verla en función de grafo:\n",
        "\n",
        "<img src=\"https://image.ibb.co/j258ud/rnn_graph.png\" alt=\"rnn_graph\" border=\"0\" width=\"600\">\n",
        "\n",
        "¡Y así es como funcionan!\n",
        "\n",
        "\n",
        "Todo esto está sacado de este video en inglés que os recomiendo encarecidamente que veais tantas veces como sea necesario para asimilar y asentar lo que os acabo de explicar: https://www.youtube.com/watch?v=UNmqTiOnRfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DckLwzz-cXuR"
      },
      "source": [
        "### ¿Y para qué se usan las RNN?\n",
        "\n",
        "Pues existen varios tipos:\n",
        "\n",
        "<img src=\"https://image.ibb.co/cRTptJ/rnn_types.png\" alt=\"rnn_types\" border=\"0\">\n",
        "\n",
        "Como hemos comentado, son muy buenas sobretodo cuando nuestros datos son secuenciales:\n",
        "\n",
        "* Predicción de acciones en bolsa\n",
        " * Los valores de una acción dependen en gran medida de los valores que tenía anteriormente\n",
        "* Generación de secuencias\n",
        " * Siempre que nuestros datos sean secuencias y un dato en un instante $t$ dependa del dato en el instante $t-1$\n",
        "* Generación de texto\n",
        " * Por ejemplo, cuando el movil te sugiere palabras. Se fija en la ultima palabra que has escrito, y en las letras que estás escribiendo en ese momento para sugerirte las próximas letras o incluso palabras\n",
        "* Reconocimiento de voz\n",
        " * En este caso tenemos la anterior palabra reconocida, y el audio que nos llega en ese momento\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PL_ZGJVqbna"
      },
      "source": [
        "## **3. Long Short-Term Memory networks**\n",
        "\n",
        "Ahora que ya sabéis cómo funcionan las redes recurrentes, vamos a ver las más famosas, las LSTM. Esta es la estructura de una RNN:\n",
        "\n",
        "<img src=\"https://image.ibb.co/mUUR7d/rnn_1.png\" alt=\"rnn_1\" border=\"0\">\n",
        "\n",
        "Pero antes, ¿por qué son las LSTM las más usadas? \n",
        "\n",
        "Resulta que las RNN convencionales tienen problemas de memoria. Paradójico, ¿no creéis? Que las redes especialemente diseñadas para recordar sean incapaces de recordar a largo plazo. ¿Y por qué esto es un problema?\n",
        "\n",
        "Pues volviendo al problema de nuestro roommate, para este ejemplo solo necesitamos conocer lo que comimos ayer, así que no pasaría nada. \n",
        "\n",
        "<img src=\"https://image.ibb.co/n6kNfy/rnn_2.png\" alt=\"rnn_2\" border=\"0\">\n",
        "\n",
        "Pero imagináos que en vez de un menú con 3 comidas, tuviese 60 platos.\n",
        "\n",
        "<img src=\"https://image.ibb.co/mCWv0y/rnn_3.png\" alt=\"rnn_3\" border=\"0\">\n",
        "\n",
        "Las RNN convencionales no serían capaces de recordar cosas que pasaron hace mucho tiempo. Sin embargo, las LSTM sí!\n",
        "\n",
        "\n",
        "Echémosle un ojo a la arquitectura de las RNN y de las LSTM:\n",
        "\n",
        "### RNN\n",
        "\n",
        "<img src=\"https://image.ibb.co/mkAind/rnn_rnn_arch.png\" alt=\"rnn_rnn_arch\" border=\"0\">\n",
        "\n",
        "### LSTM\n",
        "\n",
        "<img src=\"https://image.ibb.co/hw4LYJ/rnn_lstm_arch.png\" alt=\"rnn_lstm_arch\" border=\"0\">\n",
        "\n",
        "Resulta que las RNN son una cadena de módulos repetidos, en los que cada módulo puede ser tan simple como una tanh de función de activación. Las LSTM tienen esta estructura de cadena de módulos, pero esta vez el módulo es un poco más complejo. Cada módulo, en vez de tener una simple función de activación, existen 4 neuronas que interactúan entre ellas.\n",
        "\n",
        "Vamos a tratar de entenderlo, pero primero, os explico la nomenclatura:\n",
        "\n",
        "<img src=\"https://image.ibb.co/na6W7d/rnn_arch_nomenclature.png\" alt=\"rnn_arch_nomenclature\" border=\"0\" height=\"100\">\n",
        "\n",
        "En los diagramas de arriba, por cada línea viaja un vector, desde la salida de un nodo hasta las entradas de otros. Los círculos rosas indican operaciones elemento a elemento, como sumas de vectores, mientras que las cajas amarillas son capas neuronales que se aprenden al entrenar. Las líneas que se unen indican concatenar, y las que se separan, que el mismo contenido de la linea viaja a dos destinos distintos.\n",
        "\n",
        "### La idea clave de las LSTMs\n",
        "\n",
        "La clave es el estado de la celda, que está indicado en el diagrama como la línea que viaja por la parte de arriba: \n",
        "\n",
        "<img src=\"https://image.ibb.co/hvJYLy/rnn_arch_cell_status.png\" alt=\"rnn_arch_cell_status\" border=\"0\">\n",
        "\n",
        "El estado de la celda es como una especie de cinta transportadora que viaja a lo largo de toda la arquitectura de la red con muy pocas interacciones (y las pocaas que tiene son lineales): lo cual implica que la información simplemente fluye sin ser modificada.\n",
        "\n",
        "La parte ingeniosa es que las capas de la LSTM pueden (o no) aportar información a esta cinta transportadora, y esa decisión la toman las \"puertas\":\n",
        "\n",
        "<img src=\"https://image.ibb.co/kjHSfy/rnn_arch_gate.png\" alt=\"rnn_arch_gate\" border=\"0\">\n",
        "\n",
        "Las puertas no son otra cosa que una forma de regular cuidadosamente la información que llega a la cinta transportadora. Están compuestas de una red neuronal con una activación de tipo sigmoide y una multiplicación elemento a elemento.\n",
        "\n",
        "Así, la capa sigmoide da como salida un número entre 0 y uno, que implica cómo de importante es esa información para dejarla pasr a la cinta transportadora. Un 0 significa que no me importa, y un uno significa que es muy importante.\n",
        "\n",
        "Como podéis ver en el diagrama, una LSTM tiene 3 puertas de este tipo, para proteger y controlar la cina transportadora. Podemos entender rápidamente su funcionamiento ayudándonos de este blog: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "Y este blog también es muy interesante, para vosotros, curiosos: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "**Visto esto, vamos a ver qué es lo que pueden hacer las Redes Recurrentes!**\n",
        "\n",
        "APLICACIONES: https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-32-mNGlgW"
      },
      "source": [
        "## **4. Ejemplos de aplicación**\n",
        "### 4.1. Clasificación de imágenes con MNIST\n",
        "Vamos a ver si podemos clasificar las imágenes del MNIST con una LSTM, y con qué precisión:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kQESzO8Gpp3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import initializers\n",
        "  \n",
        "# Hyper parameters\n",
        "batch_size = 128\n",
        "nb_epoch = 10\n",
        "\n",
        "# Parameters for MNIST dataset\n",
        "img_rows, img_cols = 28, 28\n",
        "nb_classes = 10\n",
        "\n",
        "# Parameters for LSTM network\n",
        "nb_lstm_outputs = 30\n",
        "nb_time_steps = img_rows\n",
        "dim_input_vector = img_cols\n",
        "\n",
        "# Load MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print('X_train original shape:', X_train.shape)\n",
        "input_shape = (nb_time_steps, dim_input_vector)\n",
        "\n",
        "X_train = X_train.astype('float32') / 255.\n",
        "X_test = X_test.astype('float32') / 255.\n",
        "\n",
        "Y_train = to_categorical(y_train, nb_classes)\n",
        "Y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj3Wd_h9GyZL"
      },
      "outputs": [],
      "source": [
        "# Construimos la LSTM\n",
        "##Code ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5OK7xlWGzy6"
      },
      "outputs": [],
      "source": [
        "# Entrenamos\n",
        "##Code ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EftAupsOHHEi",
        "outputId": "1ef7ad76-9839-4633-d6d0-255af73e1c47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 1s 5ms/step - loss: 0.7039 - accuracy: 0.7842\n",
            "Summary: Loss over the test dataset: 0.70, Accuracy: 0.78\n"
          ]
        }
      ],
      "source": [
        "# Evaluamos\n",
        "evaluation = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=1)\n",
        "print('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sujpiHLLNgLB"
      },
      "outputs": [],
      "source": [
        "# Fuente: https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-2-f7e5ece849f5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o95JMK1Ijcxd"
      },
      "source": [
        "### 4.2. Predicción de letras consecutivas en el alfabeto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRO44I4sjg9i"
      },
      "outputs": [],
      "source": [
        "# Naive LSTM to learn one-char to one-char mapping (https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/)\n",
        "import numpy\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# define the raw dataset\n",
        "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "# create mapping of characters to integers (0-25) and the reverse\n",
        "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
        "\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 1\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, len(alphabet) - seq_length, 1):\n",
        "\t##Code ##\n",
        "\tprint(seq_in, '->', seq_out)\n",
        "\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = ## Code ##\n",
        "\n",
        "# normalize\n",
        "X = #code#\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = ## Code ##\n",
        "\n",
        "# create and fit the model\n",
        "##Code ##\n",
        "\n",
        "# summarize performance of the model\n",
        "scores = model.evaluate(X, y, verbose=0)\n",
        "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "# demonstrate some model predictions\n",
        "for pattern in dataX:\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(len(alphabet))\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tprint(seq_in, \"->\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO_UiqtWkxXh"
      },
      "source": [
        "### 4.3. Predicción de letras consecutivas en el alfabeto teniendo como input grupos de tres letras\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQr4cr-Akt0p"
      },
      "outputs": [],
      "source": [
        "# PEGAR CÓDIGO ANTERIOR Y MODIFICAR PARA QUE LA SECUENCIA DE ENTRADA SEA 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPmo9dUbTY5p"
      },
      "source": [
        "### 4.4. Ejemplos de Time Series prediction con LSTMs\n",
        "\n",
        "Ejemplo 1. Vamos a tratar de predecir el número viajeros de una aerolinea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dgLCBBnqbrA"
      },
      "outputs": [],
      "source": [
        "# LSTM for international airline passengers problem with regression framing\n",
        "# https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
        "!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/airline/international-airline-passengers.csv\n",
        "\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import read_csv\n",
        "import math\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "  ## Code##\n",
        "\treturn numpy.array(dataX), numpy.array(dataY)\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "\n",
        "# load the dataset\n",
        "dataframe = read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
        "dataset = dataframe.values\n",
        "dataset = dataset.astype('float32')\n",
        "\n",
        "# normalize the dataset\n",
        "## Code##\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(dataset) * 0.67)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = ## Code##\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "look_back = 1\n",
        "trainX, trainY = ## Code##\n",
        "testX, testY = ## Code##\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = ## Code ##\n",
        "testX = ## Code ##\n",
        "\n",
        "# create and fit the LSTM network\n",
        "## Code ##\n",
        "\n",
        "# make predictions\n",
        "trainPredict = ## Code ##\n",
        "testPredict = ## Code ##\n",
        "\n",
        "# invert predictions\n",
        "## Code##\n",
        "\n",
        "# calculate root mean squared error\n",
        "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "print('Test Score: %.2f RMSE' % (testScore))\n",
        "\n",
        "# shift train predictions for plotting\n",
        "trainPredictPlot = numpy.empty_like(dataset)\n",
        "trainPredictPlot[:, :] = numpy.nan\n",
        "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = numpy.empty_like(dataset)\n",
        "testPredictPlot[:, :] = numpy.nan\n",
        "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
        "\n",
        "# plot baseline and predictions\n",
        "plt.plot(scaler.inverse_transform(dataset))\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIZShF-PTdFp"
      },
      "source": [
        "\n",
        "\n",
        "Ejemplo 2. Vamos a tratar de solucionar el problema del IMDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iii7Rca2orFW"
      },
      "outputs": [],
      "source": [
        "# imdb problem\n",
        "# https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
        "  \n",
        "# LSTM for sequence classification in the IMDB dataset\n",
        "import numpy\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = ## Code ##\n",
        "\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = ## Code ##\n",
        "X_test = ## Code ##\n",
        "\n",
        "# create the model\n",
        "## Code ##\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_Tlzmh31gDl"
      },
      "source": [
        "\n",
        "\n",
        "Ejemplo 3. Vamos a tratar de predecir el número de ventas de un champú.\n",
        " Para ello, disponemos de un dataset en el que se incluyen las ventas de los últimos 3 años."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54oMSRo74Kow"
      },
      "outputs": [],
      "source": [
        "!rm shampo*\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlRG2K783dpK"
      },
      "outputs": [],
      "source": [
        "!wget -O shampoo-sales.csv https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import DataFrame\n",
        "from pandas import Series\n",
        "from pandas import concat\n",
        "from pandas import read_csv\n",
        "from pandas import datetime\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from math import sqrt\n",
        "from matplotlib import pyplot\n",
        "import numpy\n",
        "\n",
        "# date-time parsing function for loading the dataset\n",
        "def parser(x):\n",
        "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
        "\n",
        "# frame a sequence as a supervised learning problem\n",
        "def timeseries_to_supervised(data, lag=1):\n",
        "\tdf = DataFrame(data)\n",
        "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
        "\tcolumns.append(df)\n",
        "\tdf = concat(columns, axis=1)\n",
        "\tdf.fillna(0, inplace=True)\n",
        "\treturn df\n",
        "\n",
        "# create a differenced series\n",
        "def difference(dataset, interval=1):\n",
        "\tdiff = list()\n",
        "\tfor i in range(interval, len(dataset)):\n",
        "\t\tvalue = dataset[i] - dataset[i - interval]\n",
        "\t\tdiff.append(value)\n",
        "\treturn Series(diff)\n",
        "\n",
        "# invert differenced value\n",
        "def inverse_difference(history, yhat, interval=1):\n",
        "\treturn yhat + history[-interval]\n",
        "\n",
        "# scale train and test data to [-1, 1]\n",
        "def scale(train, test):\n",
        "\t# fit scaler\n",
        "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\tscaler = scaler.fit(train)\n",
        "\t# transform train\n",
        "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
        "\ttrain_scaled = scaler.transform(train)\n",
        "\t# transform test\n",
        "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
        "\ttest_scaled = scaler.transform(test)\n",
        "\treturn scaler, train_scaled, test_scaled\n",
        "\n",
        "# inverse scaling for a forecasted value\n",
        "def invert_scale(scaler, X, value):\n",
        "\tnew_row = [x for x in X] + [value]\n",
        "\tarray = numpy.array(new_row)\n",
        "\tarray = array.reshape(1, len(array))\n",
        "\tinverted = scaler.inverse_transform(array)\n",
        "\treturn inverted[0, -1]\n",
        "\n",
        "# fit an LSTM network to training data\n",
        "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
        "## Code ##\n",
        "\n",
        "# make a one-step forecast\n",
        "def forecast_lstm(model, batch_size, X):\n",
        "\t## Code ##\n",
        "\treturn yhat[0,0]"
      ],
      "metadata": {
        "id": "JU9HFHx_ca0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "series = read_csv('shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
        "print('Dataset cargado')\n",
        "\n",
        "# transform data to be stationary\n",
        "raw_values = series.values\n",
        "diff_values = ## Code ##\n",
        "print('Datos transformados')\n",
        "\n",
        "# transform data to be supervised learning\n",
        "supervised = ## Code ##\n",
        "supervised_values = supervised.values\n",
        "print('Datos transformados a supervisado')\n",
        "\n",
        "# split data into train and test-sets\n",
        "train, test = supervised_values[0:-12], supervised_values[-12:]\n",
        "\n",
        "# transform the scale of the data\n",
        "scaler, train_scaled, test_scaled = scale(train, test)\n",
        "print('Datos escalados')\n",
        "\n",
        "# fit the model\n",
        "lstm_model = ## Code ##\n",
        "# forecast the entire training dataset to build up state for forecasting\n",
        "train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
        "lstm_model.predict(train_reshaped, batch_size=1)"
      ],
      "metadata": {
        "id": "SdtPlyMhWSjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# walk-forward validation on the test data\n",
        "predictions = list()\n",
        "for i in range(len(test_scaled)):\n",
        "\t# make one-step forecast\n",
        "\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
        "\tyhat = forecast_lstm(lstm_model, 1, X)\n",
        "\t# invert scaling\n",
        "\tyhat = invert_scale(scaler, X, yhat)\n",
        "\t# invert differencing\n",
        "\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
        "\t# store forecast\n",
        "\tpredictions.append(yhat)\n",
        "\texpected = raw_values[len(train) + i + 1]\n",
        "\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
        "\n",
        "# report performance\n",
        "rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
        "print('Test RMSE: %.3f' % rmse)\n",
        "# line plot of observed vs predicted\n",
        "pyplot.plot(raw_values[-12:])\n",
        "pyplot.plot(predictions)\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "jO0sas4SWWoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8OIf8z6HzAD"
      },
      "source": [
        "### 4.5. Ejemplo de generación de música"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4l8nxP7H1Pc",
        "outputId": "a5fdae27-45e9-463f-b358-c163d8d9c807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Classical-Piano-Composer'...\n",
            "remote: Enumerating objects: 334, done.\u001b[K\n",
            "remote: Total 334 (delta 0), reused 0 (delta 0), pack-reused 334\u001b[K\n",
            "Receiving objects: 100% (334/334), 721.79 MiB | 37.40 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n",
            "total 24\n",
            "drwxr-xr-x 1 root root 4096 May 11 13:53 .\n",
            "drwxr-xr-x 1 root root 4096 May 11 13:49 ..\n",
            "drwxr-xr-x 6 root root 4096 May 11 13:53 Classical-Piano-Composer\n",
            "drwxr-xr-x 4 root root 4096 May  3 13:41 .config\n",
            "drwxr-xr-x 1 root root 4096 May  3 13:42 sample_data\n",
            "-rw-r--r-- 1 root root  519 May 11 13:50 shampoo-sales.csv\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Skuldur/Classical-Piano-Composer\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfP7LkprH30K"
      },
      "outputs": [],
      "source": [
        "# FUENTE: https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5\n",
        "\n",
        "\"\"\" This module prepares midi file data and feeds it to the neural\n",
        "    network for training \"\"\"\n",
        "import glob\n",
        "import pickle\n",
        "import numpy\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def train_network():\n",
        "    \"\"\" Train a Neural Network to generate music \"\"\"\n",
        "    notes = get_notes()\n",
        "\n",
        "    # get amount of pitch names\n",
        "    n_vocab = len(set(notes))\n",
        "\n",
        "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
        "\n",
        "    model = create_network(network_input, n_vocab)\n",
        "\n",
        "    train(model, network_input, network_output)\n",
        "\n",
        "def get_notes():\n",
        "    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n",
        "    notes = []\n",
        "\n",
        "    for file in glob.glob(\"Classical-Piano-Composer/midi_songs/*.mid\"):\n",
        "        midi = converter.parse(file)\n",
        "\n",
        "        print(\"Parsing %s\" % file)\n",
        "\n",
        "        notes_to_parse = None\n",
        "\n",
        "        try: # file has instrument parts\n",
        "            s2 = instrument.partitionByInstrument(midi)\n",
        "            notes_to_parse = s2.parts[0].recurse() \n",
        "        except: # file has notes in a flat structure\n",
        "            notes_to_parse = midi.flat.notes\n",
        "\n",
        "        for element in notes_to_parse:\n",
        "            if isinstance(element, note.Note):\n",
        "                notes.append(str(element.pitch))\n",
        "            elif isinstance(element, chord.Chord):\n",
        "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "\n",
        "    with open('Classical-Piano-Composer/data/notes', 'wb') as filepath:\n",
        "        pickle.dump(notes, filepath)\n",
        "\n",
        "    return notes\n",
        "\n",
        "def prepare_sequences(notes, n_vocab):\n",
        "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "    sequence_length = 100\n",
        "\n",
        "    # get all pitch names\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "     # create a dictionary to map pitches to integers\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "\n",
        "    # create input sequences and the corresponding outputs\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        network_output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # reshape the input into a format compatible with LSTM layers\n",
        "    network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    # normalize input\n",
        "    network_input = network_input / float(n_vocab)\n",
        "\n",
        "    network_output = to_categorical(network_output)\n",
        "\n",
        "    return (network_input, network_output)\n",
        "\n",
        "def create_network(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(\n",
        "        512,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "        return_sequences=True\n",
        "    ))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(512, return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(512))\n",
        "    model.add(Dense(256))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "    return model\n",
        "\n",
        "def train(model, network_input, network_output):\n",
        "    \"\"\" train the neural network \"\"\"\n",
        "    filepath = \"Classical-Piano-Composer/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath,\n",
        "        monitor='loss',\n",
        "        verbose=0,\n",
        "        save_best_only=True,\n",
        "        mode='min'\n",
        "    )\n",
        "    callbacks_list = [checkpoint]\n",
        "\n",
        "    model.fit(network_input, network_output, epochs=1, batch_size=64, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuQF5vaDINDe"
      },
      "outputs": [],
      "source": [
        "train_network()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zl2tYSwTqwF",
        "outputId": "ab54c487-49b3-4cf9-d44c-85ee292f1a6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 43837652 May 11 13:53 Classical-Piano-Composer/new_weights.hdf5\n",
            "-rw-r--r-- 1 root root 43699036 May 11 13:53 Classical-Piano-Composer/weights.hdf5\n",
            "-rw-r--r-- 1 root root 43860224 May 11 13:56 Classical-Piano-Composer/weights-improvement-01-4.7510-bigger.hdf5\n"
          ]
        }
      ],
      "source": [
        "!ls -la Classical-Piano-Composer/*.hdf5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8lFJy7Lenjh"
      },
      "outputs": [],
      "source": [
        "weights_path = 'Classical-Piano-Composer/weights-improvement-01-4.7510-bigger.hdf5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGfP38GKIVqw"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm \n",
        "\n",
        "def generate():\n",
        "    \"\"\" Generate a piano midi file \"\"\"\n",
        "    #load the notes used to train the model\n",
        "    with open('Classical-Piano-Composer/data/notes', 'rb') as filepath:\n",
        "        notes = pickle.load(filepath)\n",
        "\n",
        "    # Get all pitch names\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "    # Get all pitch names\n",
        "    n_vocab = len(set(notes))\n",
        "\n",
        "    network_input, normalized_input = prepare_sequences(notes, pitchnames, n_vocab)\n",
        "    model = create_network(normalized_input, n_vocab)\n",
        "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
        "    output_midi = create_midi(prediction_output)\n",
        "    return output_midi\n",
        "    \n",
        "def prepare_sequences(notes, pitchnames, n_vocab):\n",
        "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "    # map between notes and integers and back\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    sequence_length = 100\n",
        "    network_input = []\n",
        "    output = []\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # reshape the input into a format compatible with LSTM layers\n",
        "    normalized_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    # normalize input\n",
        "    normalized_input = normalized_input / float(n_vocab)\n",
        "\n",
        "    return (network_input, normalized_input)\n",
        "  \n",
        "def create_network(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(\n",
        "        512,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "        return_sequences=True\n",
        "    ))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(512, return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(512))\n",
        "    model.add(Dense(256))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "    # Load the weights to each node\n",
        "    model.load_weights(weights_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
        "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
        "    # pick a random sequence from the input as a starting point for the prediction\n",
        "    start = numpy.random.randint(0, len(network_input)-1)\n",
        "\n",
        "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    pattern = network_input[start]\n",
        "    prediction_output = []\n",
        "\n",
        "    # generate 500 notes\n",
        "    for note_index in tqdm(range(500)):\n",
        "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "        prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "        prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "        index = numpy.argmax(prediction)\n",
        "        result = int_to_note[index]\n",
        "        prediction_output.append(result)\n",
        "\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "\n",
        "    return prediction_output\n",
        "\n",
        "def create_midi(prediction_output):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    midi_stream.write('midi', fp='Classical-Piano-Composer/test_output.mid')\n",
        "    \n",
        "    return output_notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyBZ3SzEIkFm"
      },
      "outputs": [],
      "source": [
        "output_notes = generate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCpcVvStYYMN"
      },
      "outputs": [],
      "source": [
        "output_notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sA3fcLBeR-N9"
      },
      "outputs": [],
      "source": [
        "!ls -la Classical-Piano-Composer/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LwcpURqVMPB"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('Classical-Piano-Composer/test_output.mid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_-TxJQrnmDa"
      },
      "source": [
        "Para escuchar MIDIs creados con una red entrenada: https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL9sFwLqTkI8"
      },
      "source": [
        "Y aquí tenéis algunos otros ejemplos muy interesantes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4wCC0vt3PPV"
      },
      "outputs": [],
      "source": [
        "# Ejemplo de trading\n",
        "# https://github.com/happynoom/DeepTrade_keras\n",
        "\n",
        "# Ejemplo de cómo generar el 6o libro de GOT\n",
        "# https://github.com/zackthoutt/got-book-6\n",
        "\n",
        "# Ejemplo de detección de anuncios en videos\n",
        "# https://github.com/harvitronix/continuous-online-video-classification-blog"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BONUS**\n",
        "\n",
        "Transformer: https://towardsdatascience.com/transformers-141e32e69591 \n",
        "\n",
        "Visual transformers: https://theaisummer.com/vision-transformer/"
      ],
      "metadata": {
        "id": "M6JO8-WJXRMt"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}